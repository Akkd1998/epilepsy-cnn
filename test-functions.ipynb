{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "\n",
    "X_train = np.random.randint(1,10, size=(2,20,1)).astype(np.float32)\n",
    "Y_train = np.array([[1,0,0],[0,0,1]]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[4.]\n",
      "  [7.]\n",
      "  [7.]\n",
      "  [1.]\n",
      "  [9.]\n",
      "  [5.]\n",
      "  [8.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [8.]\n",
      "  [2.]\n",
      "  [6.]\n",
      "  [8.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [5.]\n",
      "  [7.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]]\n",
      "\n",
      " [[8.]\n",
      "  [1.]\n",
      "  [6.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [5.]\n",
      "  [5.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [5.]\n",
      "  [7.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [6.]\n",
      "  [8.]\n",
      "  [5.]\n",
      "  [4.]\n",
      "  [2.]]]\n",
      "\n",
      "\n",
      "\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print('\\n\\n')\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates Tensorflow placeholders that act for input data and their labels\n",
    "    \n",
    "    Arguments:\n",
    "    n_x - no. of features for X\n",
    "    n_x - no. of classes for Y\n",
    "    \n",
    "    Returns:\n",
    "    X - placeholder for data that contains input featurs,\n",
    "        shape: (no. of examples, no. of features). No. of examples is set to None\n",
    "    Y - placeholder for data that contains output class labels,\n",
    "        shape (no. of examples, no. of classes). No. of examples is set ot None\n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, name='X', shape=(None, n_x, 1))\n",
    "    Y = tf.placeholder(tf.float32, name='Y', shape=(None, n_y))\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2. 1. 0.]]\n",
      "\n",
      " [[2. 1. 2.]]\n",
      "\n",
      " [[0. 2. 1.]]\n",
      "\n",
      " [[1. 1. 1.]]\n",
      "\n",
      " [[2. 1. 0.]]]\n",
      "[[[ 2.]\n",
      "  [-1.]\n",
      "  [ 1.]]\n",
      "\n",
      " [[ 1.]\n",
      "  [ 2.]\n",
      "  [-1.]]\n",
      "\n",
      " [[-2.]\n",
      "  [-1.]\n",
      "  [ 0.]]\n",
      "\n",
      " [[ 0.]\n",
      "  [-2.]\n",
      "  [ 2.]]]\n",
      "[[[ 1.  2.]]\n",
      "\n",
      " [[-1. -1.]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "c1w = np.random.randint(0,3,size=(5,1,3)).astype(np.float32)\n",
    "print(c1w)\n",
    "\n",
    "np.random.seed(7)\n",
    "c2w = np.random.randint(-2,3,size=(4,3,1)).astype(np.float32)\n",
    "print(c2w)\n",
    "\n",
    "np.random.seed(8)\n",
    "c3w = np.random.randint(-2,3,size=(2,1,2)).astype(np.float32)\n",
    "print(c3w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():\n",
    "    parameters = {}\n",
    "    parameters['CONV1_W'] = tf.get_variable('CONV1_W', initializer=tf.constant(c1w))\n",
    "    parameters['CONV1_Str'] = 3\n",
    "    \n",
    "    parameters['CONV2_W'] = tf.get_variable('CONV2_W', initializer=tf.constant(c2w))\n",
    "    parameters['CONV2_Str'] = 2\n",
    "    \n",
    "    parameters['CONV3_W'] = tf.get_variable('CONV3_W', initializer=tf.constant(c3w))\n",
    "    parameters['CONV3_Str'] = 2\n",
    "    \n",
    "    parameters['FC1_units'] = 2\n",
    "    \n",
    "    parameters['DO_prob'] = 0.5\n",
    "    \n",
    "    parameters['output_classes'] = 2\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, training=False):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    (CONV BN RELU) -> (CONV BN RELU) -> (CONV BN RELU) -> (FC RELU DROPOUT) -> FC\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters\n",
    "                  \"CONV1_W\", \"CONV2_W\", \"CONV3_W\", \"FC1_units\", \"DO_prob\", \"output_classes\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit (without softmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    CONV1_W = parameters['CONV1_W']\n",
    "    CONV1_Str = parameters['CONV1_Str']\n",
    "    CONV2_W = parameters['CONV2_W']\n",
    "    CONV2_Str = parameters['CONV2_Str']\n",
    "    CONV3_W = parameters['CONV3_W']\n",
    "    CONV3_Str = parameters['CONV3_Str']\n",
    "    FC1_units = parameters['FC1_units']\n",
    "    DO_prob = parameters['DO_prob']\n",
    "    output_classes = parameters[\"output_classes\"]\n",
    "    \n",
    "    \n",
    "    #Layer 1\n",
    "    # CONV\n",
    "    Z1 = tf.nn.conv1d(X, CONV1_W, stride=CONV1_Str, padding='VALID', data_format='NWC', name='conv1')\n",
    "    # Batch Normalization\n",
    "    B1 = tf.contrib.layers.batch_norm(Z1, is_training=training)\n",
    "#     # RELU\n",
    "#     A1 = tf.nn.relu(B1)\n",
    "    \n",
    "#     #Layer 2\n",
    "#     # CONV\n",
    "#     Z2 = tf.nn.conv1d(A1, CONV2_W, stride=CONV2_Str, padding='VALID', data_format='NWC', name='conv2')\n",
    "#     # Batch Normalization\n",
    "#     B2 = tf.contrib.layers.batch_norm(Z2, is_training=training)\n",
    "#     # RELU\n",
    "#     A2 = tf.nn.relu(Z2)\n",
    "    \n",
    "#     #Layer 3\n",
    "#     # CONV\n",
    "#     Z3 = tf.nn.conv1d(A2, CONV3_W, stride=CONV3_Str, padding='VALID', data_format='NWC', name='conv3')\n",
    "#     # Batch Normalization\n",
    "#     B3 = tf.contrib.layers.batch_norm(Z3, is_training=training)\n",
    "#     # RELU\n",
    "#     A3 = tf.nn.relu(Z3)\n",
    "    \n",
    "#     # Flatten activations for FC layer\n",
    "#     A3_flat = tf.contrib.layers.flatten(A3)\n",
    "    \n",
    "#     # Layer 4\n",
    "#     # FC\n",
    "#     A4 = tf.contrib.layers.fully_connected(A3_flat, FC1_units, activation_fn=tf.nn.relu)\n",
    "#     # Dropout\n",
    "#     A4_dropped = tf.contrib.layers.dropout(A4, keep_prob=DO_prob, is_training=training)\n",
    "    \n",
    "#     # Layer 5\n",
    "#     # FC\n",
    "#     A5 = tf.contrib.layers.fully_connected(A4_dropped, output_classes, activation_fn=None)\n",
    "    \n",
    "    return B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.3994527   2.0976143   0.7139616 ]\n",
      "  [-0.26476097  0.63132095  2.0908873 ]\n",
      "  [-0.26476097 -1.3237371  -0.96894777]\n",
      "  [-0.26476097  1.1200852   0.10199451]\n",
      "  [ 0.79428387 -0.3462081  -1.2749313 ]\n",
      "  [ 0.0378232  -0.83497286  0.2549863 ]]\n",
      "\n",
      " [[-1.6263905  -0.83497286 -1.2749313 ]\n",
      "  [-2.2315593  -1.3237371  -0.8159561 ]\n",
      "  [ 0.79428387  0.14255619 -0.20398903]\n",
      "  [ 0.34040737  0.14255619  0.7139616 ]\n",
      "  [ 0.642992   -0.3462081  -0.5099726 ]\n",
      "  [ 0.642992    0.87570286  1.1729367 ]]]\n"
     ]
    }
   ],
   "source": [
    "ops.reset_default_graph()\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "parameters = get_params()\n",
    "\n",
    "with tf.Session() as sess, tf.control_dependencies(update_ops):\n",
    "    \n",
    "    X, Y = create_placeholders(20, 3)\n",
    "    f = forward_propagation(X, parameters, training=True)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    sess.run(init)\n",
    "    sess.run(init_l)\n",
    "    \n",
    "    result, _ = sess.run([f, update_ops], feed_dict={X: X_train, Y: Y_train})\n",
    "    \n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.666666666666668\n",
      "56.19213072624631\n",
      "[ 1.41182284 -0.77709576 -0.63472707]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([100,-23,-15])\n",
    "print(np.mean(a))\n",
    "print(np.std(a))\n",
    "\n",
    "print((a - np.mean(a))/np.std(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.conv2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "batch_norm() got an unexpected keyword argument 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-993c44d15f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m41\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mbn_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bn1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bn1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: batch_norm() got an unexpected keyword argument 'name'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.Session() as sess, tf.control_dependencies(update_ops):\n",
    "    x = tf.constant(np.array([[41, 100],[21,-23]]).astype(np.float32))\n",
    "    bn_train = tf.contrib.layers.batch_norm(x, is_training=True, name=\"bn1\")\n",
    "    \n",
    "    bn = tf.contrib.layers.batch_norm(x, is_training=False, name=\"bn1\")\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    print(sess.run([bn_train, bn]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
