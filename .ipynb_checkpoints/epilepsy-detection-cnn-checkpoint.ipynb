{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing modules\n",
    "\n",
    "Let's first add these libraries to our project:\n",
    "\n",
    "`numpy`: for matrix operations\n",
    "\n",
    "`tensorlfow`: deep learning layers\n",
    "\n",
    "`maplotlitb`: visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py as h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for solving the problem\n",
    "\n",
    "<ol>\n",
    "    <li>Read data and format it.</li>\n",
    "    <li>Use sliding window approach to augment data.</li>\n",
    "    <li>Split data into training/dev/test sets.</li>\n",
    "    <li>Create procedure for randomly initializing parameters with specified shape using Xavier's initialization.</li>\n",
    "    <li>Create convolution and pooling procedures.</li>\n",
    "    <li>Implement forward propagation.</li>\n",
    "    <li>Implement cost function.</li>\n",
    "    <li>Create model (uses Adam optimizer for minimization).</li>\n",
    "    <li>Train model.</li>\n",
    "    <li>Hyperparameter tuning using cross-validation sets.</li>\n",
    "    <li>Retrain model until higher accuracy is achevied.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "\n",
    "The sets in the dataset are divided into five categories.\n",
    "\n",
    "\n",
    "SET A:\tZ directory with\tZ000.txt - Z100.txt<br>\n",
    "SET B: \tO directory with\tO000.txt - O100.txt<br>\n",
    "SET C:\tN directory with\tN000.txt - N100.txt<br>\n",
    "SET D:\tF directory\twith\tF000.txt - F100.txt<br>\n",
    "SET E:\tS directory with\tS000.txt - S100.txt<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot labels\n",
    "\n",
    "Let's convert the dataset which is in dictionary format, to a single giant training set which contains all classes and a label set represented in one-hot format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset that is already stored in the file\n",
    "\n",
    "class_map = { 'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4 }\n",
    "\n",
    "def read_dataset_and_one_hot_labels(filepath, setname):\n",
    "    data = { }\n",
    "\n",
    "    with h5.File(filepath, 'r') as aug_file:\n",
    "\n",
    "        no_of_classes = len(aug_file[setname].keys())\n",
    "\n",
    "        for s in aug_file[setname].keys():\n",
    "            for i in range(len(aug_file[setname][s])):\n",
    "                if (('X_' + setname) in data):\n",
    "                    data['X_train'] = np.append(data['X_' + setname], [aug_file[setname][s][i]], axis=0)\n",
    "                    data['Y_' + setname + '_classname'] = np.append(data['Y_' + setname + '_classname'], [class_map[s]], axis=0)\n",
    "                else:\n",
    "                    data['X_' + setname] = np.array([aug_file['train'][s][i]])\n",
    "                    data['Y_' + setname + '_classname'] = np.array([class_map[s]])\n",
    "\n",
    "\n",
    "        data['Y_' + setname] = tf.one_hot(data['Y_' + setname + '_classname'], depth=no_of_classes, axis=-1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "WRITE TEXT HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(parameter_shapes):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow using Xaviar's initialization.\n",
    "    The shapes are:\n",
    "    parameter_shapes: a dictionary where keys represent tensorflow variable names, and values\n",
    "    are shapes of the parameters in a list format\n",
    "    Returns:\n",
    "    params -- a dictionary of tensors containing parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    params = { }\n",
    "    \n",
    "    for n,s in parameter_shapes.items():\n",
    "        param = tf.get_variable(n, s, initializer = tf.contrib.layers.xavier_initializer())\n",
    "        params[n] = param\n",
    "    \n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "WRITE TEXT HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, training=False):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    (CONV BN RELU) -> (CONV BN RELU) -> (CONV BN RELU) -> (FC RELU DROPOUT) -> FC\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters\n",
    "                  \"CONV1_W\", \"CONV2_W\", \"CONV3_W\", \"FC1_units\", \"DO_prob\", \"output_classes\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit (without softmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    CONV1_W = parameters['CONV1_W']\n",
    "    CONV2_W = parameters['CONV2_W']\n",
    "    CONV3_W = parameters['CONV3_W']\n",
    "    FC1_units = parameters['FC1_units']\n",
    "    DO_prob = parameters['DO_prob']\n",
    "    output_classes = parameters[\"output_classes\"]\n",
    "    \n",
    "    \n",
    "    #Layer 1\n",
    "    # CONV\n",
    "    Z1 = tf.nn.conv1d(X, CONV1_W, stride=CONV1_Str, padding='VALID', data_format='NCW')\n",
    "    # Batch Normalization\n",
    "    B1 = tf.contrib.layers.batch_norm(Z1, is_training=training)\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    \n",
    "    #Layer 2\n",
    "    # CONV\n",
    "    Z2 = tf.nn.conv1d(A1, CONV2_W, stride=CONV2_Str, padding='VALID', data_format='NCW')\n",
    "    # Batch Normalization\n",
    "    B2 = tf.contrib.layers.batch_norm(Z2, is_training=training)\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    \n",
    "    #Layer 3\n",
    "    # CONV\n",
    "    Z3 = tf.nn.conv1d(A2, CONV3_W, stride=CONV3_Str, padding='VALID', data_format='NCW')\n",
    "    # Batch Normalization\n",
    "    B3 = tf.contrib.layers.batch_norm(Z3, is_training=training)\n",
    "    # RELU\n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    \n",
    "    # Flatten activations for FC layer\n",
    "    A3_flat = tf.contrib.layers.flatten(A3)\n",
    "    \n",
    "    # Layer 4\n",
    "    # FC\n",
    "    A4 = tf.contrib.layers.fully_connected(A3_flat, FC1_units, activation_fn=tf.nn.relu)\n",
    "    # Dropout\n",
    "    A4_dropped = tf.contrib.layers.dropout(A4, keep_prob=DO_prob, is_training=training)\n",
    "    \n",
    "    # Layer 5\n",
    "    # FC\n",
    "    A5 = tf.contrib.layers.fully_connected(A4_dropped, output_units, activation_fn=None)\n",
    "    \n",
    "    return A5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.conv1d?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing cost function\n",
    "\n",
    "WRITE TEXT HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, Y, parameters, training=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply softmax to the output classes and find cross entropy loss\n",
    "    X - Input data\n",
    "    Y - One-hot output class training labels\n",
    "    \n",
    "    Returns:\n",
    "    cost - cross entropy loss\n",
    "    \"\"\"\n",
    "    \n",
    "    Y_hat = forward_propation(X, parameters, training=training)\n",
    "    \n",
    "    cost = tf.nn.softmax_cross_entropy_with_logits_v2(logits=Y_hat, labels=Y)\n",
    "    \n",
    "    return cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
