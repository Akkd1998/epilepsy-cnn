{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing modules\n",
    "\n",
    "Let's first add these libraries to our project:\n",
    "\n",
    "`numpy`: for matrix operations\n",
    "\n",
    "`tensorlfow`: deep learning layers\n",
    "\n",
    "`maplotlitb`: visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py as h5\n",
    "from tensorflow.python.framework import ops\n",
    "from os import path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for solving the problem\n",
    "\n",
    "<ol>\n",
    "    <li>Read data and format it.</li>\n",
    "    <li>Use sliding window approach to augment data.</li>\n",
    "    <li>Split data into training/dev/test sets.</li>\n",
    "    <li>Create procedure for randomly initializing parameters with specified shape using Xavier's initialization.</li>\n",
    "    <li>Create convolution and pooling procedures.</li>\n",
    "    <li>Implement forward propagation.</li>\n",
    "    <li>Implement cost function.</li>\n",
    "    <li>Create model (uses Adam optimizer for minimization).</li>\n",
    "    <li>Train model.</li>\n",
    "    <li>Hyperparameter tuning using cross-validation sets.</li>\n",
    "    <li>Retrain model until higher accuracy is achevied.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "\n",
    "The sets in the dataset are divided into five categories.\n",
    "\n",
    "\n",
    "SET A:\tZ directory with\tZ000.txt - Z100.txt<br>\n",
    "SET B: \tO directory with\tO000.txt - O100.txt<br>\n",
    "SET C:\tN directory with\tN000.txt - N100.txt<br>\n",
    "SET D:\tF directory\twith\tF000.txt - F100.txt<br>\n",
    "SET E:\tS directory with\tS000.txt - S100.txt<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_relative_path = 'dataset/random-iter-3/'\n",
    "\n",
    "datafile = dataset_relative_path + 'datafile1024.h5'\n",
    "\n",
    "with h5.File(datafile, 'r') as datafile:\n",
    "    X_train = np.array(datafile['X_train'])\n",
    "    Y_train = np.array(datafile['Y_train'])\n",
    "    \n",
    "    X_dev = np.array(datafile['X_dev'])\n",
    "    Y_dev = np.array(datafile['Y_dev'])\n",
    "    \n",
    "    X_test = np.array(datafile['X_test'])\n",
    "    Y_test = np.array(datafile['Y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dimensions_compatible(arr):\n",
    "    \n",
    "    return arr.reshape(arr.shape[0],-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = make_dimensions_compatible(X_train)\n",
    "X_dev = make_dimensions_compatible(X_dev)\n",
    "X_test = make_dimensions_compatible(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11250, 1024, 1)\n",
      "(11250, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 1000\n",
    "X_dev = X_dev / 1000\n",
    "X_test = X_test / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "WRITE TEXT HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(parameter_shapes, parameter_values = {}):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow using Xaviar's initialization.\n",
    "    The parameters are:\n",
    "    parameter_shapes: a dictionary where keys represent tensorflow variable names, and values\n",
    "    are shapes of the parameters in a list format\n",
    "    Returns:\n",
    "    params -- a dictionary of tensors containing parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    params = { }\n",
    "    \n",
    "    for n,s in parameter_shapes.items():\n",
    "        param = tf.get_variable(n, s, initializer = tf.contrib.layers.xavier_initializer())\n",
    "        params[n] = param\n",
    "    \n",
    "    for n,v in parameter_values.items():\n",
    "        params[n] = v\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "WRITE TEXT HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_cnn1(X, parameters, training=False):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    (CONV BN RELU) -> (CONV BN RELU) -> (CONV BN RELU) -> (FC RELU DROPOUT) -> FC\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters\n",
    "                  \"CONV1_W\", \"CONV2_W\", \"CONV3_W\", \"FC1_units\", \"DO_prob\", \"output_classes\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit (without softmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    CONV1_W = parameters['CONV1_W']\n",
    "    CONV1_Str = parameters['CONV1_Str']\n",
    "    CONV2_W = parameters['CONV2_W']\n",
    "    CONV2_Str = parameters['CONV2_Str']\n",
    "    CONV3_W = parameters['CONV3_W']\n",
    "    CONV3_Str = parameters['CONV3_Str']\n",
    "    FC1_units = parameters['FC1_units']\n",
    "    DO_prob = parameters['DO_prob']\n",
    "    output_classes = parameters[\"output_classes\"]\n",
    "    \n",
    "    \n",
    "    #Layer 1\n",
    "    # CONV\n",
    "    Z1 = tf.nn.conv1d(X, CONV1_W, stride=CONV1_Str, padding='VALID', data_format='NWC', name='conv1')\n",
    "    # Batch Normalization\n",
    "    B1 = tf.contrib.layers.batch_norm(Z1, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(B1)\n",
    "    \n",
    "    #Layer 2\n",
    "    # CONV\n",
    "    Z2 = tf.nn.conv1d(A1, CONV2_W, stride=CONV2_Str, padding='VALID', data_format='NWC', name='conv2')\n",
    "    # Batch Normalization\n",
    "    B2 = tf.contrib.layers.batch_norm(Z2, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(B2)\n",
    "    \n",
    "    #Layer 3\n",
    "    # CONV\n",
    "    Z3 = tf.nn.conv1d(A2, CONV3_W, stride=CONV3_Str, padding='VALID', data_format='NWC', name='conv3')\n",
    "    # Batch Normalization\n",
    "    B3 = tf.contrib.layers.batch_norm(Z3, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A3 = tf.nn.relu(B3)\n",
    "    \n",
    "    # Flatten activations for FC layer\n",
    "    A3_flat = tf.contrib.layers.flatten(A3)\n",
    "    \n",
    "    # Layer 4\n",
    "    # FC\n",
    "    A4 = tf.contrib.layers.fully_connected(A3_flat, FC1_units, activation_fn=tf.nn.relu)\n",
    "    # Dropout\n",
    "    A4_dropped = tf.contrib.layers.dropout(A4, keep_prob=DO_prob, is_training=training)\n",
    "    \n",
    "    # Layer 5\n",
    "    # FC\n",
    "    logits = tf.contrib.layers.fully_connected(A4_dropped, output_classes, activation_fn=None)\n",
    "    \n",
    "    # Although the cost function we use will have in-built softmax computations,\n",
    "    # for predictions it'll be feasible to have a named tensor\n",
    "    softmax_output = tf.nn.softmax(logits, name='softmax_output')\n",
    "    \n",
    "    return logits, softmax_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_cnn2(X, parameters, training=False):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    (CONV BN RELU) -> (CONV BN RELU) -> (CONV BN RELU) -> (FC RELU DROPOUT) -> FC\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters\n",
    "                  \"CONV1_W\", \"CONV2_W\", \"CONV3_W\", \"FC1_units\", \"DO_prob\", \"output_classes\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit (without softmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    CONV1_W = parameters['CONV1_W']\n",
    "    CONV1_Str = parameters['CONV1_Str']\n",
    "    CONV2_W = parameters['CONV2_W']\n",
    "    CONV2_Str = parameters['CONV2_Str']\n",
    "    CONV3_W = parameters['CONV3_W']\n",
    "    CONV3_Str = parameters['CONV3_Str']\n",
    "    FC1_units = parameters['FC1_units']\n",
    "    DO_prob = parameters['DO_prob']\n",
    "    output_classes = parameters[\"output_classes\"]\n",
    "    \n",
    "    \n",
    "    #Layer 1\n",
    "    # CONV\n",
    "    Z1 = tf.nn.conv1d(X, CONV1_W, stride=CONV1_Str, padding='VALID', data_format='NWC', name='conv1')\n",
    "    # Batch Normalization\n",
    "    B1 = tf.contrib.layers.batch_norm(Z1, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(B1)\n",
    "    \n",
    "    #Layer 2\n",
    "    # CONV\n",
    "    Z2 = tf.nn.conv1d(A1, CONV2_W, stride=CONV2_Str, padding='VALID', data_format='NWC', name='conv2')\n",
    "    # Batch Normalization\n",
    "    B2 = tf.contrib.layers.batch_norm(Z2, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(B2)\n",
    "    \n",
    "    #Layer 3\n",
    "    # CONV\n",
    "    Z3 = tf.nn.conv1d(A2, CONV3_W, stride=CONV3_Str, padding='VALID', data_format='NWC', name='conv3')\n",
    "    # Batch Normalization\n",
    "    B3 = tf.contrib.layers.batch_norm(Z3, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A3 = tf.nn.relu(B3)\n",
    "    \n",
    "    # Flatten activations for FC layer\n",
    "    A3_flat = tf.contrib.layers.flatten(A3)\n",
    "    \n",
    "    # Layer 4\n",
    "    # FC\n",
    "    A4 = tf.contrib.layers.fully_connected(A3_flat, FC1_units, activation_fn=tf.nn.relu)\n",
    "    # Dropout\n",
    "    A4_dropped = tf.contrib.layers.dropout(A4, keep_prob=DO_prob, is_training=training)\n",
    "    \n",
    "    # Layer 5\n",
    "    # FC\n",
    "    logits = tf.contrib.layers.fully_connected(A4_dropped, output_classes, activation_fn=None)\n",
    "    \n",
    "    # Although the cost function we use will have in-built softmax computations,\n",
    "    # for predictions it'll be feasible to have a named tensor\n",
    "    softmax_output = tf.nn.softmax(logits, name='softmax_output')\n",
    "    \n",
    "    return logits, softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_cnn3(X, parameters, training=False):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    (CONV BN RELU) -> (CONV BN RELU DROPOUT) -> (CONV BN RELU) -> (FC RELU DROPOUT) -> FC\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters\n",
    "                  \"CONV1_W\", \"CONV2_W\", \"CONV3_W\", \"FC1_units\", \"DO_prob\", \"output_classes\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit (without softmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    CONV1_W = parameters['CONV1_W']\n",
    "    CONV1_Str = parameters['CONV1_Str']\n",
    "    CONV2_W = parameters['CONV2_W']\n",
    "    CONV2_Str = parameters['CONV2_Str']\n",
    "    DO_prob_middle_layer = parameters['DO_prob_middle_layer']\n",
    "    CONV3_W = parameters['CONV3_W']\n",
    "    CONV3_Str = parameters['CONV3_Str']\n",
    "    FC1_units = parameters['FC1_units']\n",
    "    DO_prob = parameters['DO_prob']\n",
    "    output_classes = parameters[\"output_classes\"]\n",
    "    \n",
    "    \n",
    "    #Layer 1\n",
    "    # CONV\n",
    "    Z1 = tf.nn.conv1d(X, CONV1_W, stride=CONV1_Str, padding='VALID', data_format='NWC', name='conv1')\n",
    "    # Batch Normalization\n",
    "    B1 = tf.contrib.layers.batch_norm(Z1, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(B1)\n",
    "    \n",
    "    #Layer 2\n",
    "    # CONV\n",
    "    Z2 = tf.nn.conv1d(A1, CONV2_W, stride=CONV2_Str, padding='VALID', data_format='NWC', name='conv2')\n",
    "    # Batch Normalization\n",
    "    B2 = tf.contrib.layers.batch_norm(Z2, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(B2)\n",
    "    # Dropout\n",
    "    A2_dropped = tf.contrib.layers.dropout(A2, keep_prob=DO_prob_middle_layer, is_training=training)\n",
    "    \n",
    "    #Layer 3\n",
    "    # CONV\n",
    "    Z3 = tf.nn.conv1d(A2_dropped, CONV3_W, stride=CONV3_Str, padding='VALID', data_format='NWC', name='conv3')\n",
    "    # Batch Normalization\n",
    "    B3 = tf.contrib.layers.batch_norm(Z3, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A3 = tf.nn.relu(B3)\n",
    "    \n",
    "    # Flatten activations for FC layer\n",
    "    A3_flat = tf.contrib.layers.flatten(A3)\n",
    "    \n",
    "    # Layer 4\n",
    "    # FC\n",
    "    A4 = tf.contrib.layers.fully_connected(A3_flat, FC1_units, activation_fn=tf.nn.relu)\n",
    "    # Dropout\n",
    "    A4_dropped = tf.contrib.layers.dropout(A4, keep_prob=DO_prob, is_training=training)\n",
    "    \n",
    "    # Layer 5\n",
    "    # FC\n",
    "    logits = tf.contrib.layers.fully_connected(A4_dropped, output_classes, activation_fn=None)\n",
    "    \n",
    "    # Although the cost function we use will have in-built softmax computations,\n",
    "    # for predictions it'll be feasible to have a named tensor\n",
    "    softmax_output = tf.nn.softmax(logits, name='softmax_output')\n",
    "    \n",
    "    return logits, softmax_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing cost function\n",
    "\n",
    "WRITE TEXT HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, Y, parameters, nn_key, training):\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply softmax to the output classes and find cross entropy loss\n",
    "    X - Input data\n",
    "    Y - One-hot output class training labels\n",
    "    \n",
    "    Returns:\n",
    "    cost - cross entropy loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # FIXME: setting training=training causes problems during evaluation time\n",
    "    if nn_key == 'cnn1':\n",
    "        logits, Y_hat = forward_propagation_cnn1(X, parameters, training=training)\n",
    "    elif nn_key == 'cnn2':\n",
    "        logits, Y_hat = forward_propagation_cnn2(X, parameters, training=training)\n",
    "    elif nn_key == 'cnn3':\n",
    "        logits, Y_hat = forward_propagation_cnn3(X, parameters, training=training)\n",
    "    else:\n",
    "        KeyError('Provided nn_key doesn\\'t match with any model')\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "    \n",
    "    return cost, Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisites for training\n",
    "\n",
    "Here are some procedures that are necessary to execute before the actual training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create placeholders\n",
    "\n",
    "Tensorflow functions take input in the form of `feed_dict`. The variables in other functions are placeholders for the actual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates Tensorflow placeholders that act for input data and their labels\n",
    "    \n",
    "    Arguments:\n",
    "    n_x - no. of features for X\n",
    "    n_x - no. of classes for Y\n",
    "    \n",
    "    Returns:\n",
    "    X - placeholder for data that contains input featurs,\n",
    "        shape: (no. of examples, no. of features). No. of examples is set to None\n",
    "    Y - placeholder for data that contains output class labels,\n",
    "        shape (no. of examples, no. of classes). No. of examples is set ot None\n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, name='X', shape=(None, n_x, 1))\n",
    "    Y = tf.placeholder(tf.float32, name='Y', shape=(None, n_y))\n",
    "    is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "    \n",
    "    return X,Y,is_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter shapes\n",
    "\n",
    "To initialize model parameters, we've created a procedure above. It takes as an argument a dictionary in which we supply the model parameter shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_shapes(nn_key):\n",
    "    \"\"\"\n",
    "    Get tha shapes of all parameters used in the model.\n",
    "    Convolutional layer parameter shapes (filters) are in list format\n",
    "    \n",
    "    Arguments:\n",
    "    nn_key - Provide the key for the neural network model used\n",
    "    could be, 'cnn1', 'cnn2'\n",
    "    \n",
    "    Returns:\n",
    "    param_shapes - dict that contains all the parameters as follows\n",
    "    CONV1_W, CONV2_W, CONV3_W\n",
    "    param_values:\n",
    "    CONV1_Str, CONV2_Str, CONV3_Str,\n",
    "    FC1_units, DO_prob, output_classes\n",
    "    \"\"\"\n",
    "    \n",
    "    param_shapes = {}\n",
    "    param_values = {}\n",
    "    \n",
    "    do_prob = {\n",
    "        'cnn1': 0.5,\n",
    "        'cnn2': 0.7,\n",
    "        'cnn3': 0.7\n",
    "    }\n",
    "    \n",
    "    do_prob_middle_layer = {\n",
    "        'cnn1': 0, # not used\n",
    "        'cnn2': 0, # not used\n",
    "        'cnn3': 0.2\n",
    "    }\n",
    "    \n",
    "    fc1_units = {\n",
    "        'cnn1': 20,\n",
    "        'cnn2': 15,\n",
    "        'cnn3': 15\n",
    "    }\n",
    "\n",
    "    # Conv Layer 1 parameter shapes\n",
    "    # No. of channels: 24, Filter size: 5, Stride: 3\n",
    "    param_shapes['CONV1_W'] = [5, 1, 24]\n",
    "    param_values['CONV1_Str'] = 3\n",
    "    \n",
    "    # Conv Layer 2 parameter shapes\n",
    "    # No. of channels: 16, Filter size: 3, Stride: 2\n",
    "    param_shapes['CONV2_W'] = [3, 24, 16]\n",
    "    param_values['CONV2_Str'] = 2\n",
    "    \n",
    "    # Dropout after the convolutional layer 2\n",
    "    # Not used in some cases\n",
    "    param_values['DO_prob_middle_layer'] = do_prob_middle_layer[nn_key]\n",
    "    \n",
    "    # Conv Layer 3 parameter shapes\n",
    "    # No. of channels: 8, Filter size: 3, Stride: 2\n",
    "    param_shapes['CONV3_W'] = [3, 16, 8]\n",
    "    param_values['CONV3_Str'] = 2\n",
    "    \n",
    "    # Fully connected layer 1 units = 20\n",
    "    param_values['FC1_units'] = fc1_units[nn_key]\n",
    "    \n",
    "    # Dropout layer after fully connected layer 1 probability\n",
    "    param_values['DO_prob'] = do_prob[nn_key]\n",
    "    \n",
    "    # Fully connected layer 2 units (also last layer)\n",
    "    # No. of units = no. of output classes = 3\n",
    "    param_values['output_classes'] = 3\n",
    "    \n",
    "    return param_shapes, param_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random mini-batches\n",
    "\n",
    "For each epoch we'll use different sets of mini-batches to avoid any possible overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (number of examples, window size) (m, n_x)\n",
    "    Y -- output classes, of shape (number of examples, output classes) (m, n_y)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = np.floor(m/mini_batch_size).astype(int) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting costs\n",
    "\n",
    "WRITE TEXT HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_costs(costs, dev_costs, learning_rate, total_epochs):\n",
    "    # plot the cost\n",
    "    plt.plot(costs, color='blue', label='training')\n",
    "    plt.plot(dev_costs, color='green', label='dev')\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.title(\"Learning rate = %f\\nTotal Epochs = %i\" % (learning_rate, total_epochs))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data\n",
    "\n",
    "WRITE TEXT HERE...\n",
    "\n",
    "[UPDATE_OPS](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_dev, Y_dev,\n",
    "          learning_rate = 0.009, num_epochs = 100, minibatch_size = 64, print_cost = True,\n",
    "          save_session_path=None, model_file=None, restore_session=False, save_session_interval=5, max_to_keep=10,\n",
    "          nn_key='cnn1'):\n",
    "    \"\"\"\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_train -- test set, of shape (None, n_y = 6)\n",
    "    X_test -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_test -- test set, of shape (None, n_y = 6)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    (m, n_x,_) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    dev_costs = []\n",
    "    \n",
    "    model_path = None\n",
    "    if (save_session_path != None and model_file != None):\n",
    "        model_path = save_session_path + model_file\n",
    "    \n",
    "    \n",
    "    # Create Placeholders of the correct shape\n",
    "    X, Y, is_train = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    param_shapes, param_values = parameter_shapes(nn_key)\n",
    "    parameters = initialize_parameters(param_shapes, param_values)\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    # Prediction: Use Y_hat to compute the output class during prediction\n",
    "    cost, Y_hat = compute_cost(X, Y, parameters, nn_key, is_train)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    # optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # For saving / restoring sesison when training for long\n",
    "    epoch_counter = tf.get_variable('epoch_counter', shape=[], initializer=tf.zeros_initializer)\n",
    "    counter_op = tf.assign_add(epoch_counter, 1)\n",
    "    saver = tf.train.Saver(max_to_keep=max_to_keep)\n",
    "    \n",
    "    # Calculate the correct predictions\n",
    "    predict_op = tf.argmax(Y_hat, 1)\n",
    "    correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "\n",
    "    # Calculate accuracy on the test set\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    # For impementation of batch norm the tf.GraphKeys.UPDATE_OPS dependency needs to be added\n",
    "    # see documentation on tf.contrib.layers.batch_norm\n",
    "#     update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess: #, tf.control_dependencies(update_ops):\n",
    "        \n",
    "        \n",
    "        # restore the previous session if the path already exists\n",
    "        if (model_path != None and restore_session==True):\n",
    "            print(\"Restoring session...\\n\")\n",
    "            saver.restore(sess, model_path)\n",
    "            print(\"Previous epoch counter: %i\\n\\n\" % epoch_counter.eval())\n",
    "        else:\n",
    "            sess.run(init)\n",
    "        \n",
    "        tf.train.export_meta_graph(model_path + '.meta') # save the model file (.meta) only once\n",
    "        \n",
    "        print(\"Cost at start: %f\" % cost.eval({X: X_train, Y: Y_train, is_train: False}))\n",
    "        print(\"Dev cost: %f\" % cost.eval({X: X_dev, Y: Y_dev, is_train: False}))\n",
    "        \n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train, is_train: False})\n",
    "        dev_accuracy = accuracy.eval({X: X_dev, Y: Y_dev, is_train: False})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Dev Accuracy:\", dev_accuracy)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                \n",
    "                try:\n",
    "\n",
    "                    # Select a minibatch\n",
    "                    (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                    # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                    # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                    _,minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y, is_train: True})\n",
    "\n",
    "                    epoch_cost += minibatch_cost / num_minibatches\n",
    "                \n",
    "                # Implement early stopping mechanism on KeyboardInterrupt\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"KeyboardInterrupt received. Stopping early\")\n",
    "                    plot_costs(np.squeeze(costs), np.squeeze(dev_costs), learning_rate, epoch_counter.eval())\n",
    "                    return parameters\n",
    "                \n",
    "            \n",
    "            if (epoch % save_session_interval == 0 and save_session_path != None):\n",
    "                saver.save(sess, model_path, write_meta_graph=False)\n",
    "            \n",
    "            # Save the costs after each epoch for plotting learning curve\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                dev_cost = cost.eval({X: X_dev, Y: Y_dev, is_train: False})\n",
    "                dev_costs.append(dev_cost)\n",
    "                \n",
    "                \n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and (epoch + 1) % 5 == 0:\n",
    "                print (\"\\nCost after epoch %i: %f\" % (epoch + 1, epoch_cost))\n",
    "                print (\"Dev cost after epoch %i: %f\" % (epoch + 1, dev_cost))\n",
    "                \n",
    "                train_accuracy = accuracy.eval({X: X_train, Y: Y_train, is_train: False})\n",
    "                dev_accuracy = accuracy.eval({X: X_dev, Y: Y_dev, is_train: False})\n",
    "                print(\"Train Accuracy:\", train_accuracy)\n",
    "                print(\"Dev Accuracy:\", dev_accuracy)\n",
    "            \n",
    "            # increment the epoch_counter in case the session is saved\n",
    "            # and restored later\n",
    "            sess.run(counter_op)\n",
    "                \n",
    "                \n",
    "        if (save_session_path != None):\n",
    "            saver.save(sess, model_path, write_meta_graph=False)\n",
    "        \n",
    "        \n",
    "        plot_costs(np.squeeze(costs), np.squeeze(dev_costs), learning_rate, epoch_counter.eval())\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train, is_train: False})\n",
    "        dev_accuracy = accuracy.eval({X: X_dev, Y: Y_dev, is_train: False})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Dev Accuracy:\", dev_accuracy)\n",
    "                \n",
    "        return parameters\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at start: 1.098616\n",
      "Dev cost: 1.098607\n",
      "Train Accuracy: 0.288\n",
      "Dev Accuracy: 0.25714287\n",
      "KeyboardInterrupt received. Stopping early\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_dev, Y_dev,\n",
    "                   learning_rate=0.00002,\n",
    "                   num_epochs=1000,\n",
    "                   minibatch_size=64,\n",
    "                   save_session_path='train/dataset-1024-3/cnn3_lr-0.00002_mbs-64/',\n",
    "                   model_file='model',\n",
    "                   restore_session=False,\n",
    "                   save_session_interval=10,\n",
    "                   nn_key='cnn3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and restoring saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, session_path, model_file, Y_test_onehot=None):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    checkpoint_path = session_path\n",
    "    model_path = session_path + model_file\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        loader = tf.train.import_meta_graph(model_path)\n",
    "        loader.restore(sess, tf.train.latest_checkpoint(checkpoint_path))\n",
    "\n",
    "        graph = tf.get_default_graph()\n",
    "\n",
    "        X = graph.get_tensor_by_name('X:0')\n",
    "        Y = graph.get_tensor_by_name('Y:0')\n",
    "        is_train = graph.get_tensor_by_name('is_train:0')\n",
    "\n",
    "        Y_hat = graph.get_tensor_by_name('softmax_output:0')\n",
    "\n",
    "        predict_op = tf.argmax(Y_hat, 1)\n",
    "\n",
    "        y_hat_test = predict_op.eval({X: X_test, is_train: False})\n",
    "        \n",
    "        # print the accuracy of the test set if the labels are provided\n",
    "        if (Y_test_onehot is not None):\n",
    "            y_test = np.argmax(Y_test_onehot, 1)\n",
    "            print('Accuracy: %f' % (y_hat_test == y_test).mean())\n",
    "        \n",
    "\n",
    "    return y_hat_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_voting(X_test_voting, session_path, model_file):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    checkpoint_path = session_path\n",
    "    model_path = session_path + model_file\n",
    "    \n",
    "    y_hat_test_voting = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        loader = tf.train.import_meta_graph(model_path)\n",
    "        loader.restore(sess, tf.train.latest_checkpoint(checkpoint_path))\n",
    "\n",
    "        graph = tf.get_default_graph()\n",
    "\n",
    "        X = graph.get_tensor_by_name('X:0')\n",
    "        is_train = graph.get_tensor_by_name('is_train:0')\n",
    "\n",
    "        Y_hat = graph.get_tensor_by_name('softmax_output:0')\n",
    "\n",
    "        predict_op = tf.argmax(Y_hat, 1)\n",
    "        \n",
    "        classname, idx, counts = tf.unique_with_counts(predict_op)\n",
    "        predict_voting_op = tf.gather(classname, tf.argmax(counts))\n",
    "\n",
    "        # no. of training examples with the original feature size\n",
    "        m = X_test_voting.shape[0]\n",
    "        \n",
    "        # no. of split training examples of each original example\n",
    "        m_each = X_test_voting.shape[1]\n",
    "        \n",
    "        for ex in range(m):\n",
    "            x_test_voting = make_dimensions_compatible(X_test_voting[ex])\n",
    "            pred = predict_voting_op.eval({X: x_test_voting, is_train: False})\n",
    "            \n",
    "            y_hat_test_voting.append(pred)\n",
    "\n",
    "    return y_hat_test_voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from train/dataset-1024-2/cnn3_lr-0.00002_mbs-64/model-1000\n",
      "Accuracy: 1.000000\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(X_test, 'train/dataset-1024-2/cnn3_lr-0.00002_mbs-64/', 'model.meta', Y_test_onehot=Y_test)\n",
    "\n",
    "print(predictions)\n",
    "print(np.argmax(Y_test, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read test data for voting accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from train/dataset-1024-2/cnn3_lr-0.00002_mbs-64/model-1000\n",
      "Accuracy with voting: 1.000000\n"
     ]
    }
   ],
   "source": [
    "testfile = dataset_relative_path + 'testset_voting_1024.h5'\n",
    "session_path = 'train/dataset-1024-2/cnn3_lr-0.00002_mbs-64/'\n",
    "model_file = 'model.meta'\n",
    "\n",
    "with h5.File(testfile, 'r') as testfile:\n",
    "    X_test_voting = testfile['X']\n",
    "    X_test_voting = np.array(X_test_voting) / 1000\n",
    "    y_test_voting = np.array(testfile['Y'])\n",
    "    \n",
    "    y_hat_test_voting = predict_voting(X_test_voting, session_path, model_file)\n",
    "    \n",
    "    print(\"Accuracy with voting: %f\" % (y_test_voting == y_hat_test_voting).mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat_test_voting)\n",
    "print(y_test_voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
