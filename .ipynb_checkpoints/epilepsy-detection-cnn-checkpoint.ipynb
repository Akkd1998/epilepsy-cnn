{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing modules\n",
    "\n",
    "Let's first add these libraries to our project:\n",
    "\n",
    "`numpy`: for matrix operations\n",
    "\n",
    "`tensorlfow`: deep learning layers\n",
    "\n",
    "`maplotlitb`: visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py as h5\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for solving the problem\n",
    "\n",
    "<ol>\n",
    "    <li>Read data and format it.</li>\n",
    "    <li>Use sliding window approach to augment data.</li>\n",
    "    <li>Split data into training/dev/test sets.</li>\n",
    "    <li>Create procedure for randomly initializing parameters with specified shape using Xavier's initialization.</li>\n",
    "    <li>Create convolution and pooling procedures.</li>\n",
    "    <li>Implement forward propagation.</li>\n",
    "    <li>Implement cost function.</li>\n",
    "    <li>Create model (uses Adam optimizer for minimization).</li>\n",
    "    <li>Train model.</li>\n",
    "    <li>Hyperparameter tuning using cross-validation sets.</li>\n",
    "    <li>Retrain model until higher accuracy is achevied.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "\n",
    "The sets in the dataset are divided into five categories.\n",
    "\n",
    "\n",
    "SET A:\tZ directory with\tZ000.txt - Z100.txt<br>\n",
    "SET B: \tO directory with\tO000.txt - O100.txt<br>\n",
    "SET C:\tN directory with\tN000.txt - N100.txt<br>\n",
    "SET D:\tF directory\twith\tF000.txt - F100.txt<br>\n",
    "SET E:\tS directory with\tS000.txt - S100.txt<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = \"dataset/datafile512.h5\"\n",
    "\n",
    "with h5.File(datafile, 'r') as datafile:\n",
    "    X_train = np.array(datafile['X_train'])\n",
    "    Y_train = np.array(datafile['Y_train'])\n",
    "    \n",
    "    X_dev = np.array(datafile['X_dev'])\n",
    "    Y_dev = np.array(datafile['Y_dev'])\n",
    "    \n",
    "    X_test = np.array(datafile['X_test'])\n",
    "    Y_test = np.array(datafile['Y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dimensions_compatible(arr):\n",
    "    \n",
    "    return arr.reshape(arr.shape[0],-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = make_dimensions_compatible(X_train)\n",
    "X_dev = make_dimensions_compatible(X_dev)\n",
    "X_test = make_dimensions_compatible(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25650, 512, 1)\n",
      "(25650, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 1000\n",
    "X_dev = X_dev / 1000\n",
    "X_test = X_test / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "WRITE TEXT HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(parameter_shapes, parameter_values = {}):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow using Xaviar's initialization.\n",
    "    The parameters are:\n",
    "    parameter_shapes: a dictionary where keys represent tensorflow variable names, and values\n",
    "    are shapes of the parameters in a list format\n",
    "    Returns:\n",
    "    params -- a dictionary of tensors containing parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    params = { }\n",
    "    \n",
    "    for n,s in parameter_shapes.items():\n",
    "        param = tf.get_variable(n, s, initializer = tf.contrib.layers.xavier_initializer())\n",
    "        params[n] = param\n",
    "    \n",
    "    for n,v in parameter_values.items():\n",
    "        params[n] = v\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "WRITE TEXT HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, training=False):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    (CONV BN RELU) -> (CONV BN RELU) -> (CONV BN RELU) -> (FC RELU DROPOUT) -> FC\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters\n",
    "                  \"CONV1_W\", \"CONV2_W\", \"CONV3_W\", \"FC1_units\", \"DO_prob\", \"output_classes\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit (without softmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    CONV1_W = parameters['CONV1_W']\n",
    "    CONV1_Str = parameters['CONV1_Str']\n",
    "    CONV2_W = parameters['CONV2_W']\n",
    "    CONV2_Str = parameters['CONV2_Str']\n",
    "    CONV3_W = parameters['CONV3_W']\n",
    "    CONV3_Str = parameters['CONV3_Str']\n",
    "    FC1_units = parameters['FC1_units']\n",
    "    DO_prob = parameters['DO_prob']\n",
    "    output_classes = parameters[\"output_classes\"]\n",
    "    \n",
    "    \n",
    "    #Layer 1\n",
    "    # CONV\n",
    "    Z1 = tf.nn.conv1d(X, CONV1_W, stride=CONV1_Str, padding='VALID', data_format='NWC')\n",
    "    # Batch Normalization\n",
    "    B1 = tf.contrib.layers.batch_norm(Z1, is_training=training)\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(B1)\n",
    "    \n",
    "    #Layer 2\n",
    "    # CONV\n",
    "    Z2 = tf.nn.conv1d(A1, CONV2_W, stride=CONV2_Str, padding='VALID', data_format='NWC')\n",
    "    # Batch Normalization\n",
    "    B2 = tf.contrib.layers.batch_norm(Z2, is_training=training)\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(B2)\n",
    "    \n",
    "    #Layer 3\n",
    "    # CONV\n",
    "    Z3 = tf.nn.conv1d(A2, CONV3_W, stride=CONV3_Str, padding='VALID', data_format='NWC')\n",
    "    # Batch Normalization\n",
    "    B3 = tf.contrib.layers.batch_norm(Z3, is_training=training)\n",
    "    # RELU\n",
    "    A3 = tf.nn.relu(B3)\n",
    "    \n",
    "    # Flatten activations for FC layer\n",
    "    A3_flat = tf.contrib.layers.flatten(A3)\n",
    "    \n",
    "    # Layer 4\n",
    "    # FC\n",
    "    A4 = tf.contrib.layers.fully_connected(A3_flat, FC1_units, activation_fn=tf.nn.relu)\n",
    "    # Dropout\n",
    "    A4_dropped = tf.contrib.layers.dropout(A4, keep_prob=DO_prob, is_training=training)\n",
    "    \n",
    "    # Layer 5\n",
    "    # FC\n",
    "    A5 = tf.contrib.layers.fully_connected(A4_dropped, output_classes, activation_fn=None)\n",
    "    \n",
    "    return A5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.conv1d?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing cost function\n",
    "\n",
    "WRITE TEXT HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, Y, parameters, training=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply softmax to the output classes and find cross entropy loss\n",
    "    X - Input data\n",
    "    Y - One-hot output class training labels\n",
    "    \n",
    "    Returns:\n",
    "    cost - cross entropy loss\n",
    "    \"\"\"\n",
    "    \n",
    "    Y_hat = forward_propagation(X, parameters, training=training)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Y_hat, labels=Y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisites for training\n",
    "\n",
    "Here are some procedures that are necessary to execute before the actual training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create placeholders\n",
    "\n",
    "Tensorflow functions take input in the form of `feed_dict`. The variables in other functions are placeholders for the actual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates Tensorflow placeholders that act for input data and their labels\n",
    "    \n",
    "    Arguments:\n",
    "    n_x - no. of features for X\n",
    "    n_x - no. of classes for Y\n",
    "    \n",
    "    Returns:\n",
    "    X - placeholder for data that contains input featurs,\n",
    "        shape: (no. of examples, no. of features). No. of examples is set to None\n",
    "    Y - placeholder for data that contains output class labels,\n",
    "        shape (no. of examples, no. of classes). No. of examples is set ot None\n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, name='X', shape=(None, n_x, 1))\n",
    "    Y = tf.placeholder(tf.float32, name='Y', shape=(None, n_y))\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter shapes\n",
    "\n",
    "To initialize model parameters, we've created a procedure above. It takes as an argument a dictionary in which we supply the model parameter shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_shapes():\n",
    "    \"\"\"\n",
    "    Get tha shapes of all parameters used in the model.\n",
    "    Convolutional layer parameter shapes (filters) are in list format\n",
    "    \n",
    "    Returns:\n",
    "    param_shapes - dict that contains all the parameters as follows\n",
    "    CONV1_W, CONV2_W, CONV3_W\n",
    "    param_values:\n",
    "    CONV1_Str, CONV2_Str, CONV3_Str,\n",
    "    FC1_units, DO_prob, output_classes\n",
    "    \"\"\"\n",
    "    \n",
    "    param_shapes = {}\n",
    "    param_values = {}\n",
    "\n",
    "    # Conv Layer 1 parameter shapes\n",
    "    # No. of channels: 24, Filter size: 5, Stride: 3\n",
    "    param_shapes['CONV1_W'] = [5, 1, 24]\n",
    "    param_values['CONV1_Str'] = 3\n",
    "    \n",
    "    # Conv Layer 2 parameter shapes\n",
    "    # No. of channels: 16, Filter size: 3, Stride: 2\n",
    "    param_shapes['CONV2_W'] = [3, 24, 16]\n",
    "    param_values['CONV2_Str'] = 2\n",
    "    \n",
    "    # Conv Layer 3 parameter shapes\n",
    "    # No. of channels: 8, Filter size: 3, Stride: 2\n",
    "    param_shapes['CONV3_W'] = [3, 16, 8]\n",
    "    param_values['CONV3_Str'] = 2\n",
    "    \n",
    "    # Fully connected layer 1 units = 20\n",
    "    param_values['FC1_units'] = 20\n",
    "    \n",
    "    # Dropout layer after fully connected layer 1 probability\n",
    "    param_values['DO_prob'] = 0.5\n",
    "    \n",
    "    # Fully connected layer 2 units (also last layer)\n",
    "    # No. of units = no. of output classes = 3\n",
    "    param_values['output_classes'] = 3\n",
    "    \n",
    "    return param_shapes, param_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random mini-batches\n",
    "\n",
    "For each epoch we'll use different sets of mini-batches to avoid any possible overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (number of examples, window size) (m, n_x)\n",
    "    Y -- output classes, of shape (number of examples, output classes) (m, n_y)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = np.floor(m/mini_batch_size).astype(int) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data\n",
    "\n",
    "WRITE TEXT HERE...\n",
    "\n",
    "[UPDATE_OPS](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_costs(costs, learning_rate):\n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_dev, Y_dev, learning_rate = 0.009,\n",
    "          num_epochs = 100, minibatch_size = 64, print_cost = True):\n",
    "    \"\"\"\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_train -- test set, of shape (None, n_y = 6)\n",
    "    X_test -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_test -- test set, of shape (None, n_y = 6)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    (m, n_x,_) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of the correct shape\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    param_shapes, param_values = parameter_shapes()\n",
    "    parameters = initialize_parameters(param_shapes, param_values)\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    cost = compute_cost(X, Y, parameters, training=True)\n",
    "    \n",
    "    # Prediction: compute the class of the example from from forward propagation\n",
    "    # Note: not used for training\n",
    "    Y_hat = forward_propagation(X, parameters, training=True)\n",
    "    \n",
    "    # For impementation of batch norm the tf.GraphKeys.UPDATE_OPS dependency needs to be added\n",
    "    # see documentation on tf.contrib.layers.batch_norm\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    # optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Calculate the correct predictions\n",
    "    predict_op = tf.argmax(Y_hat, 1)\n",
    "    correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "\n",
    "    # Calculate accuracy on the test set\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess, tf.control_dependencies(update_ops):\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        dev_accuracy = accuracy.eval({X: X_dev, Y: Y_dev})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Dev Accuracy:\", dev_accuracy)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                \n",
    "                try:\n",
    "\n",
    "                    # Select a minibatch\n",
    "                    (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                    # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                    # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                    _,minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "\n",
    "                    epoch_cost += minibatch_cost / num_minibatches\n",
    "                \n",
    "                # Implement early stopping mechanism on KeyboardInterrupt\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"KeyboardInterrupt received. Stopping early\")\n",
    "                    plot_costs(np.squeeze(costs), learning_rate)\n",
    "                    return parameters\n",
    "                \n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "                \n",
    "                train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "                dev_accuracy = accuracy.eval({X: X_dev, Y: Y_dev})\n",
    "                print(\"Train Accuracy:\", train_accuracy)\n",
    "                print(\"Dev Accuracy:\", dev_accuracy)\n",
    "                \n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        plot_costs(np.squeeze(costs), learning_rate)\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "#         predict_op = tf.argmax(Y_hat, 1)\n",
    "#         correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "#         # Calculate accuracy on the test set\n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        dev_accuracy = accuracy.eval({X: X_dev, Y: Y_dev})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Dev Accuracy:\", dev_accuracy)\n",
    "                \n",
    "        return parameters\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-275-75c4ff8772b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-274-3820b3cbbc85>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_dev, Y_dev, learning_rate, num_epochs, minibatch_size, print_cost)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Calculate accuracy on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mdev_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_dev\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m     \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4935\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4936\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4937\u001b[0;31m       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n\u001b[0m\u001b[1;32m   4938\u001b[0m                        \u001b[0;34m\"session is registered. Use `with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4939\u001b[0m                        \u001b[0;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_dev, Y_dev, learning_rate=0.00002, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CONV1_W': <tf.Variable 'CONV1_W:0' shape=(5, 1, 24) dtype=float32_ref>,\n",
       " 'CONV2_W': <tf.Variable 'CONV2_W:0' shape=(3, 24, 16) dtype=float32_ref>,\n",
       " 'CONV3_W': <tf.Variable 'CONV3_W:0' shape=(3, 16, 8) dtype=float32_ref>,\n",
       " 'CONV1_Str': 3,\n",
       " 'CONV2_Str': 2,\n",
       " 'CONV3_Str': 2,\n",
       " 'FC1_units': 20,\n",
       " 'DO_prob': 0.5,\n",
       " 'output_classes': 3}"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Failed to convert object of type <class 'dict'> to Tensor. Contents: {'CONV1_W': <tf.Variable 'CONV1_W:0' shape=(5, 1, 24) dtype=float32_ref>, 'CONV2_W': <tf.Variable 'CONV2_W:0' shape=(3, 24, 16) dtype=float32_ref>, 'CONV3_W': <tf.Variable 'CONV3_W:0' shape=(3, 16, 8) dtype=float32_ref>, 'CONV1_Str': 3, 'CONV2_Str': 2, 'CONV3_Str': 2, 'FC1_units': 20, 'DO_prob': 0.5, 'output_classes': 3}. Consider casting elements to a supported type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     60\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 61\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got {'CONV1_W': <tf.Variable 'CONV1_W:0' shape=(5, 1, 24) dtype=float32_ref>, 'CONV2_W': <tf.Variable 'CONV2_W:0' shape=(3, 24, 16) dtype=float32_ref>, 'CONV3_W': <tf.Variable 'CONV3_W:0' shape=(3, 16, 8) dtype=float32_ref>, 'CONV1_Str': 3, 'CONV2_Str': 2, 'CONV3_Str': 2, 'FC1_units': 20, 'DO_prob': 0.5, 'output_classes': 3}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-269-0af0a1691af0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"parameters\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msave_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"parameters-0.00002.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msaver_sess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver_sess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msave_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[1;32m   1279\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[1;32m   1280\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[1;32m   1328\u001b[0m           \u001b[0mrestore_sequentially\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_sequentially\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m           \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m           build_save=build_save, build_restore=build_restore)\n\u001b[0m\u001b[1;32m   1331\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m       \u001b[0;31m# Since self._name is used as a name_scope by builder(), we are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build_internal\u001b[0;34m(self, names_to_saveables, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, filename, build_save, build_restore)\u001b[0m\n\u001b[1;32m    754\u001b[0m                        \" when eager execution is not enabled.\")\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m     \u001b[0msaveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ValidateAndSliceInputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_to_keep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m       \u001b[0mmax_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_ValidateAndSliceInputs\u001b[0;34m(self, names_to_saveables)\u001b[0m\n\u001b[1;32m    661\u001b[0m                            \u001b[0;31m# Avoid comparing ops, sort only by name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m                            key=lambda x: x[0]):\n\u001b[0;32m--> 663\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mconverted_saveable_object\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaveableObjectsForOp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AddSaveable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaveables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverted_saveable_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msaveables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mSaveableObjectsForOp\u001b[0;34m(op, name)\u001b[0m\n\u001b[1;32m    625\u001b[0m           \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_element\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m           \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_convert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mBaseSaverBuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IsVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m           raise TypeError(\"names_to_saveables must be a dict mapping string \"\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    215\u001b[0m                                          as_ref=False):\n\u001b[1;32m    216\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    194\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    195\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 196\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    197\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    523\u001b[0m       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\u001b[1;32m    524\u001b[0m                       \u001b[0;34m\"Contents: %s. Consider casting elements to a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m                       \"supported type.\" % (type(values), values))\n\u001b[0m\u001b[1;32m    526\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'CONV1_W': <tf.Variable 'CONV1_W:0' shape=(5, 1, 24) dtype=float32_ref>, 'CONV2_W': <tf.Variable 'CONV2_W:0' shape=(3, 24, 16) dtype=float32_ref>, 'CONV3_W': <tf.Variable 'CONV3_W:0' shape=(3, 16, 8) dtype=float32_ref>, 'CONV1_Str': 3, 'CONV2_Str': 2, 'CONV3_Str': 2, 'FC1_units': 20, 'DO_prob': 0.5, 'output_classes': 3}. Consider casting elements to a supported type."
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver({\"parameters\": parameters})\n",
    "save_filename = \"parameters-0.00002.ckpt\"\n",
    "with tf.Session() as saver_sess:\n",
    "    save_path = saver.save(saver_sess, \"train/\" + save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[ 0.1643085 ,  0.02832928,  0.00959623, -0.1577815 ,\n",
      "          0.18924351, -0.01189184, -0.01866665, -0.0223477 ,\n",
      "         -0.08435522,  0.14691184, -0.02915606,  0.11074157,\n",
      "          0.20260207,  0.20071696, -0.1872986 ,  0.12060685,\n",
      "         -0.01182598,  0.12479548,  0.15796684,  0.0713618 ,\n",
      "         -0.09553804,  0.09870629, -0.08830424, -0.02101901]],\n",
      "\n",
      "       [[-0.16887933,  0.16461147, -0.17514198,  0.0358382 ,\n",
      "          0.12785394, -0.0477768 , -0.09394044, -0.06481086,\n",
      "          0.20126636, -0.14165503,  0.09850214,  0.19882835,\n",
      "          0.19975846,  0.03520782, -0.0587478 , -0.07248875,\n",
      "          0.11210789,  0.17382033, -0.17019679, -0.13251658,\n",
      "         -0.05781738,  0.08353408,  0.15923844, -0.03437892]],\n",
      "\n",
      "       [[-0.01887622, -0.00399101, -0.03537577, -0.00912465,\n",
      "         -0.05754562,  0.13235386, -0.21564794, -0.09492114,\n",
      "          0.10254969,  0.11964042, -0.16168648, -0.10999942,\n",
      "          0.05496974,  0.01924798,  0.134133  ,  0.18148993,\n",
      "         -0.05380638,  0.0466698 ,  0.21100672,  0.11605178,\n",
      "         -0.15948132,  0.19808991,  0.03279395, -0.04511204]],\n",
      "\n",
      "       [[-0.1531505 ,  0.1360371 ,  0.17552187, -0.072657  ,\n",
      "          0.10390188,  0.2064978 , -0.2060738 , -0.00251307,\n",
      "          0.10316496,  0.1708117 , -0.18439575,  0.05672057,\n",
      "         -0.07450461,  0.15146177,  0.06291531, -0.15118815,\n",
      "         -0.01062907, -0.0947226 ,  0.00894162, -0.19388433,\n",
      "         -0.10948235, -0.01592223,  0.17893966, -0.02725476]],\n",
      "\n",
      "       [[-0.04409654,  0.1685492 , -0.11375062, -0.09052035,\n",
      "          0.14411877, -0.11179066, -0.04502575, -0.17790776,\n",
      "         -0.17113179,  0.16160111,  0.01867341, -0.10262924,\n",
      "         -0.18486305, -0.11137895,  0.20629053,  0.10506319,\n",
      "          0.04275133, -0.04294497, -0.1365886 , -0.08690064,\n",
      "         -0.05178347,  0.10505648,  0.09812795,  0.12511526]]],\n",
      "      dtype=float32), array([[[ 0.03454825, -0.07327656, -0.18433392, ...,  0.01384151,\n",
      "         -0.0376384 , -0.21241197],\n",
      "        [ 0.0264369 ,  0.09737429, -0.1662314 , ...,  0.01771265,\n",
      "          0.02779716,  0.2148824 ],\n",
      "        [ 0.09074643,  0.14955875,  0.10030431, ..., -0.17436606,\n",
      "          0.1132088 , -0.19364998],\n",
      "        ...,\n",
      "        [ 0.13966292,  0.09236509, -0.02404495, ..., -0.06317516,\n",
      "         -0.04378319,  0.17765734],\n",
      "        [-0.19157821,  0.15578228,  0.0189836 , ...,  0.0289264 ,\n",
      "         -0.11181726,  0.15346962],\n",
      "        [-0.15033194, -0.13815138, -0.06678775, ...,  0.20982501,\n",
      "         -0.08370359, -0.00093867]],\n",
      "\n",
      "       [[ 0.19088665, -0.02133679,  0.14586759, ...,  0.05037734,\n",
      "         -0.00991294,  0.19605312],\n",
      "        [ 0.04065531, -0.17088324, -0.00174026, ..., -0.14537781,\n",
      "          0.19790578,  0.12841159],\n",
      "        [ 0.03534409,  0.14828753,  0.08637324, ...,  0.1203579 ,\n",
      "         -0.02153736, -0.14773005],\n",
      "        ...,\n",
      "        [ 0.20435509,  0.12785128,  0.17734796, ...,  0.14683509,\n",
      "          0.07817471,  0.12274739],\n",
      "        [ 0.19478527, -0.08566473, -0.08895856, ..., -0.00590713,\n",
      "          0.05258766,  0.07641312],\n",
      "        [ 0.1245679 , -0.06362133,  0.01101671, ...,  0.08452523,\n",
      "          0.20335132,  0.0348587 ]],\n",
      "\n",
      "       [[-0.09215297,  0.00598225, -0.08681248, ..., -0.18807061,\n",
      "          0.10308492,  0.14881563],\n",
      "        [ 0.05426043,  0.05483988,  0.01513442, ...,  0.15597942,\n",
      "          0.18479496,  0.01043727],\n",
      "        [ 0.21021515, -0.07014853, -0.01030777, ...,  0.03580993,\n",
      "          0.13476515, -0.11279953],\n",
      "        ...,\n",
      "        [ 0.10904738, -0.04719312,  0.16810265, ...,  0.19397241,\n",
      "         -0.05813323, -0.21560493],\n",
      "        [ 0.12123638,  0.13133243, -0.05182946, ..., -0.02149242,\n",
      "         -0.16068082,  0.20637628],\n",
      "        [ 0.04394537, -0.11068022, -0.10675342, ..., -0.11137413,\n",
      "         -0.06647837,  0.01350191]]], dtype=float32), array([[[ 1.02950722e-01,  1.18730545e-01,  1.07318997e-01,\n",
      "         -8.23232234e-02,  7.22426176e-02,  2.65023172e-01,\n",
      "          6.90088570e-02, -2.83344179e-01],\n",
      "        [-2.46273652e-01,  1.98538214e-01,  2.46038437e-01,\n",
      "         -1.82627946e-01, -1.52395800e-01, -2.63979852e-01,\n",
      "         -2.27670670e-02, -1.25692457e-01],\n",
      "        [-3.07884812e-02, -3.99737954e-02, -5.51048666e-02,\n",
      "          6.06123507e-02, -2.14976013e-01, -2.05507413e-01,\n",
      "          5.10406494e-02, -9.43745077e-02],\n",
      "        [-2.23446503e-01, -1.58778533e-01,  3.96336615e-02,\n",
      "         -1.51973620e-01, -1.27636433e-01, -1.76890522e-01,\n",
      "         -2.90343761e-02,  2.11624682e-01],\n",
      "        [ 3.37957442e-02,  8.81038904e-02,  9.72135067e-02,\n",
      "         -5.28866202e-02, -9.40107703e-02,  1.39111698e-01,\n",
      "         -1.26710251e-01,  2.06310600e-01],\n",
      "        [-9.03185457e-02,  1.20239079e-01,  2.70825744e-01,\n",
      "          6.90641999e-02, -1.40377343e-01,  1.94017887e-01,\n",
      "          7.76210725e-02,  1.92006946e-01],\n",
      "        [ 2.60322213e-01, -1.86103567e-01, -4.73356396e-02,\n",
      "          1.31012946e-01,  5.36055863e-02, -1.49103254e-01,\n",
      "          9.57444906e-02,  2.63774395e-02],\n",
      "        [ 9.58490372e-03,  5.87722957e-02,  1.55969977e-01,\n",
      "          1.74715161e-01,  1.10131979e-01, -1.79722965e-01,\n",
      "         -2.02977732e-01, -8.39007050e-02],\n",
      "        [ 9.20522809e-02,  2.81835616e-01, -7.94733018e-02,\n",
      "          1.23090357e-01,  2.76783705e-01,  1.58143133e-01,\n",
      "          2.03613669e-01, -1.57769322e-02],\n",
      "        [ 2.83651412e-01,  2.36370862e-02,  1.26903713e-01,\n",
      "         -7.21476525e-02,  2.10845917e-01, -2.62569487e-01,\n",
      "          2.28163481e-01,  3.40013802e-02],\n",
      "        [-1.71139389e-01, -2.72915065e-02,  6.28378987e-02,\n",
      "         -1.44607767e-01,  1.30800962e-01, -1.05573595e-01,\n",
      "          8.13111663e-03, -2.88549602e-01],\n",
      "        [ 7.62977600e-02,  1.03350192e-01, -2.10062087e-01,\n",
      "         -3.14284861e-02, -6.53753579e-02,  2.84851491e-02,\n",
      "          2.82338381e-01,  7.89079070e-02],\n",
      "        [ 1.15689635e-03, -2.51100361e-02,  2.55950332e-01,\n",
      "         -4.99835014e-02, -1.34336799e-01,  1.70196891e-01,\n",
      "         -2.52049506e-01,  2.71742284e-01],\n",
      "        [-1.89778984e-01, -2.40322724e-01,  2.45203614e-01,\n",
      "          2.04512477e-01, -1.79761365e-01,  1.92244381e-01,\n",
      "          1.49027675e-01, -1.68645769e-01],\n",
      "        [ 1.34132117e-01, -7.20069110e-02,  2.34913051e-01,\n",
      "         -1.96531266e-01,  1.61745042e-01, -1.94820046e-01,\n",
      "         -2.87820458e-01, -1.67443871e-01],\n",
      "        [ 6.68334961e-02,  7.14491308e-02,  9.42706466e-02,\n",
      "         -1.34689599e-01, -1.26563907e-02,  1.37816548e-01,\n",
      "          1.63856953e-01, -2.61959344e-01]],\n",
      "\n",
      "       [[ 2.17077672e-01,  1.87536240e-01,  5.84517121e-02,\n",
      "         -2.43552357e-01,  2.30777860e-01,  9.85579193e-02,\n",
      "         -1.34772271e-01,  4.11887765e-02],\n",
      "        [ 2.51484394e-01,  1.86256856e-01, -1.41883999e-01,\n",
      "          1.99499488e-01,  2.79091716e-01, -1.80168197e-01,\n",
      "          5.14957309e-02,  4.73426580e-02],\n",
      "        [-2.05032229e-01,  9.48527753e-02,  2.10830569e-02,\n",
      "         -1.31457359e-01, -5.30168414e-02, -6.81270659e-02,\n",
      "         -9.00667906e-02, -1.09920129e-01],\n",
      "        [ 9.95755792e-02, -9.47534591e-02, -1.63414761e-01,\n",
      "          8.57711136e-02,  1.30634397e-01,  1.21790111e-01,\n",
      "          2.78981864e-01, -1.73031539e-01],\n",
      "        [-8.05658400e-02,  1.26271486e-01,  2.20297575e-01,\n",
      "          1.56350374e-01, -1.73342705e-01,  1.50721192e-01,\n",
      "          2.54926443e-01, -2.31285319e-01],\n",
      "        [-5.49211055e-02,  9.10106301e-03,  4.38214242e-02,\n",
      "         -1.10613406e-01,  4.25764918e-02,  2.07217723e-01,\n",
      "          2.76342630e-02, -5.85610867e-02],\n",
      "        [-2.42733061e-02, -2.42362648e-01,  1.22362822e-01,\n",
      "          3.44445407e-02, -2.81431586e-01,  1.96363539e-01,\n",
      "          1.10555053e-01,  6.84531629e-02],\n",
      "        [ 1.63581878e-01,  2.07843691e-01,  9.27260816e-02,\n",
      "          5.88607490e-02, -1.15785301e-02, -2.88280547e-01,\n",
      "         -2.89491713e-02, -1.78664923e-04],\n",
      "        [ 1.41320199e-01, -1.34216428e-01, -4.75118309e-02,\n",
      "         -1.08882859e-01, -8.35332572e-02,  3.06995511e-02,\n",
      "          9.61816013e-02,  2.74289191e-01],\n",
      "        [-7.31465220e-03, -2.42105514e-01, -5.83755970e-03,\n",
      "          1.29757524e-02,  6.21838570e-02, -1.81705967e-01,\n",
      "         -3.61042917e-02,  2.52677202e-01],\n",
      "        [ 4.78157401e-03,  2.73520827e-01, -3.65942717e-04,\n",
      "          2.75783360e-01,  1.60678655e-01,  2.00209141e-01,\n",
      "         -9.93467420e-02,  5.61647713e-02],\n",
      "        [ 2.44768381e-01, -7.13432878e-02, -1.99440375e-01,\n",
      "         -8.72054696e-03, -5.34320623e-02, -2.78233290e-02,\n",
      "         -3.43827605e-02,  2.66410947e-01],\n",
      "        [-2.09450692e-01,  2.73653150e-01, -6.39496446e-02,\n",
      "         -8.29343945e-02,  1.84497237e-02,  8.89908969e-02,\n",
      "         -4.20951992e-02,  6.57205284e-02],\n",
      "        [ 1.27949923e-01, -1.61904514e-01,  1.06198609e-01,\n",
      "         -9.87475514e-02, -2.32993156e-01,  9.74774361e-02,\n",
      "         -2.19867304e-01, -1.29379854e-01],\n",
      "        [ 2.37938762e-02,  2.34745800e-01,  1.10285521e-01,\n",
      "         -2.70861149e-01, -2.83558518e-01, -1.96012110e-01,\n",
      "         -8.67789984e-02, -2.69347131e-02],\n",
      "        [-2.64090300e-01, -2.61319041e-01,  1.12095773e-01,\n",
      "          1.16068870e-01,  4.22601700e-02,  2.73968041e-01,\n",
      "         -4.94633764e-02, -2.16769055e-01]],\n",
      "\n",
      "       [[ 1.72984064e-01,  4.06078100e-02, -2.84014463e-01,\n",
      "         -2.15610594e-01, -2.66804099e-02,  2.12236464e-01,\n",
      "          2.35042572e-01,  9.04244184e-03],\n",
      "        [ 2.54772961e-01,  1.31989360e-01, -1.47773057e-01,\n",
      "          2.10858256e-01,  1.32705301e-01,  2.31767893e-02,\n",
      "          9.35549438e-02, -1.25227064e-01],\n",
      "        [-1.14757717e-02,  2.29272425e-01, -1.97507545e-01,\n",
      "         -2.74226576e-01,  1.32009953e-01, -1.23115152e-01,\n",
      "          1.67219907e-01,  2.51698434e-01],\n",
      "        [ 2.76752472e-01,  3.91109288e-02, -1.66488290e-02,\n",
      "          1.40302807e-01, -1.02879271e-01,  2.55242646e-01,\n",
      "         -1.76653415e-01, -5.16962856e-02],\n",
      "        [-7.53324181e-02, -3.12095881e-03,  2.09848374e-01,\n",
      "         -1.00263909e-01,  2.10619867e-02, -8.64804238e-02,\n",
      "         -9.16178375e-02,  7.63621032e-02],\n",
      "        [-3.80457938e-02, -2.65606046e-02,  5.98763525e-02,\n",
      "         -2.15495855e-01, -1.14389926e-01, -1.65411651e-01,\n",
      "          1.17951781e-01, -7.62358159e-02],\n",
      "        [ 1.41508698e-01, -2.31706798e-01, -1.71919167e-02,\n",
      "         -2.72764951e-01, -2.17244923e-01,  2.65020907e-01,\n",
      "          1.50473624e-01, -2.48908103e-02],\n",
      "        [ 7.66454637e-02, -1.13805190e-01,  2.72380292e-01,\n",
      "          1.82288170e-01, -3.45466137e-02, -1.99644864e-02,\n",
      "          7.46965408e-02,  1.13599598e-01],\n",
      "        [-2.87346184e-01,  1.38681561e-01, -1.76061869e-01,\n",
      "          6.47832453e-02, -1.17661282e-01, -1.52566269e-01,\n",
      "          3.70444357e-02,  1.03836298e-01],\n",
      "        [ 1.92352116e-01, -3.70763838e-02, -1.04354352e-01,\n",
      "          1.13566160e-01,  1.17064297e-01, -1.03444204e-01,\n",
      "          1.95107818e-01,  3.01044285e-02],\n",
      "        [-1.92425668e-01, -1.18113533e-01,  2.69835711e-01,\n",
      "          1.52462333e-01,  1.79591924e-01, -1.90701321e-01,\n",
      "         -2.15742856e-01,  1.75316900e-01],\n",
      "        [ 2.52963781e-01, -4.10325974e-02,  5.41266501e-02,\n",
      "          2.33979821e-01, -2.72689849e-01, -2.64694333e-01,\n",
      "         -1.47999018e-01,  3.24014127e-02],\n",
      "        [-8.56428891e-02,  2.21977651e-01,  7.31682777e-02,\n",
      "         -1.12265363e-01,  2.92432606e-02, -7.43502825e-02,\n",
      "         -1.02957532e-01, -1.59043863e-01],\n",
      "        [ 7.78135061e-02,  2.10855901e-02,  3.23891640e-03,\n",
      "         -2.91174650e-03,  2.83964038e-01,  2.64204383e-01,\n",
      "         -2.17408240e-02,  1.18493915e-01],\n",
      "        [-5.22500575e-02, -5.08023202e-02,  9.91666913e-02,\n",
      "          2.54862368e-01, -1.21082246e-01,  2.49309182e-01,\n",
      "         -2.51968831e-01,  2.13175237e-01],\n",
      "        [-3.27252150e-02,  2.44670153e-01,  2.23190606e-01,\n",
      "          2.18800962e-01,  2.16511488e-01,  1.55409276e-02,\n",
      "          5.58165908e-02, -1.93617180e-01]]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[ 0.0938587 , -0.07239331, -0.06818514, ...,  0.02098723,\n",
      "         0.04155301,  0.05539155],\n",
      "       [-0.01821527, -0.08697401, -0.11177664, ..., -0.10056333,\n",
      "         0.082904  , -0.02161622],\n",
      "       [-0.11454282,  0.11155944, -0.12167596, ...,  0.01112506,\n",
      "         0.00296307, -0.0262356 ],\n",
      "       ...,\n",
      "       [-0.00430894,  0.0572931 , -0.09514384, ..., -0.06464636,\n",
      "         0.1161419 ,  0.04919173],\n",
      "       [ 0.1122954 , -0.00684194, -0.02432487, ..., -0.08732492,\n",
      "         0.10387705,  0.04345487],\n",
      "       [-0.08378898, -0.01144139, -0.05281067, ..., -0.11202651,\n",
      "        -0.0297015 , -0.08832225]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32), array([[ 0.17008138, -0.29384038, -0.17582312],\n",
      "       [ 0.00385129,  0.23175842, -0.04472902],\n",
      "       [-0.21134475, -0.20951521,  0.00133795],\n",
      "       [ 0.16000634, -0.32329294, -0.09981862],\n",
      "       [ 0.23417515,  0.48110473,  0.27441633],\n",
      "       [ 0.30358166, -0.34617826, -0.04411149],\n",
      "       [-0.19648099,  0.03667688,  0.02367514],\n",
      "       [ 0.22302496,  0.2957548 ,  0.42723453],\n",
      "       [-0.00428814, -0.4516687 ,  0.39783603],\n",
      "       [ 0.22385854,  0.01701647,  0.18520135],\n",
      "       [ 0.11497879, -0.44391164,  0.27624393],\n",
      "       [-0.04469064, -0.16853267, -0.25371757],\n",
      "       [-0.3041938 ,  0.30201828, -0.22988763],\n",
      "       [ 0.48649782,  0.43864554,  0.4829765 ],\n",
      "       [ 0.2568043 ,  0.28930557,  0.15248835],\n",
      "       [ 0.03378099,  0.03613704,  0.38515252],\n",
      "       [ 0.23071128,  0.06223392,  0.36065465],\n",
      "       [-0.11145878, -0.19355574,  0.4271729 ],\n",
      "       [ 0.22493279,  0.20551085,  0.19637382],\n",
      "       [ 0.3531229 , -0.3203163 , -0.5090971 ]], dtype=float32), array([0., 0., 0.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[ 3.95124257e-02, -1.14047468e-01,  9.16563272e-02, ...,\n",
      "         9.97388959e-02,  1.15336046e-01,  4.57376689e-02],\n",
      "       [-4.13841084e-02, -8.42704922e-02, -6.64013997e-02, ...,\n",
      "        -7.04465285e-02,  8.28883052e-02, -5.17144799e-04],\n",
      "       [-7.39231706e-02, -1.72678009e-02, -3.12894583e-02, ...,\n",
      "         1.40619874e-02,  8.43380392e-03, -9.59861577e-02],\n",
      "       ...,\n",
      "       [ 9.15815383e-02, -8.19035172e-02, -9.41449031e-02, ...,\n",
      "         2.42517740e-02,  1.03577465e-01,  2.04876661e-02],\n",
      "       [-7.72297010e-02, -1.04477726e-01,  3.24738175e-02, ...,\n",
      "        -5.34086078e-02,  3.88605893e-03,  1.00998834e-01],\n",
      "       [-3.24654952e-02,  3.49085480e-02, -1.13127008e-01, ...,\n",
      "         1.20908409e-01, -1.07735395e-05,  9.57140177e-02]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32), array([[-0.29862058, -0.44624907,  0.21366   ],\n",
      "       [ 0.26665425,  0.13070041,  0.20315486],\n",
      "       [-0.37168783,  0.02024859,  0.44429207],\n",
      "       [-0.39684933,  0.2000565 , -0.17609468],\n",
      "       [-0.05052942,  0.47739136, -0.2529828 ],\n",
      "       [-0.03275603,  0.28452605,  0.37867087],\n",
      "       [ 0.48413622, -0.2487379 ,  0.2599476 ],\n",
      "       [ 0.08251607,  0.26540947,  0.503105  ],\n",
      "       [-0.08899528,  0.47121477,  0.02866066],\n",
      "       [-0.07428119, -0.4686378 ,  0.40427893],\n",
      "       [-0.03408322,  0.5034773 , -0.46455207],\n",
      "       [ 0.46196437,  0.5001771 ,  0.14000452],\n",
      "       [ 0.2928999 ,  0.35588312,  0.24679661],\n",
      "       [ 0.33063585, -0.44646436,  0.15195471],\n",
      "       [-0.06311822,  0.04519856, -0.0905686 ],\n",
      "       [-0.08188018,  0.47788906, -0.28416136],\n",
      "       [-0.18828857, -0.30228755, -0.06532854],\n",
      "       [ 0.15629607,  0.4906674 , -0.3866003 ],\n",
      "       [ 0.08491296,  0.15673262,  0.27497488],\n",
      "       [-0.32988864,  0.46208346, -0.08739921]], dtype=float32), array([0., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(train_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
