{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing modules\n",
    "\n",
    "Let's first add these libraries to our project:\n",
    "\n",
    "`numpy`: for matrix operations\n",
    "\n",
    "`tensorlfow`: deep learning layers\n",
    "\n",
    "`maplotlitb`: visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py as h5\n",
    "from tensorflow.python.framework import ops\n",
    "from os import path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for solving the problem\n",
    "\n",
    "<ol>\n",
    "    <li>Read data and format it.</li>\n",
    "    <li>Use sliding window approach to augment data.</li>\n",
    "    <li>Split data into training/dev/test sets.</li>\n",
    "    <li>Create procedure for randomly initializing parameters with specified shape using Xavier's initialization.</li>\n",
    "    <li>Create convolution and pooling procedures.</li>\n",
    "    <li>Implement forward propagation.</li>\n",
    "    <li>Implement cost function.</li>\n",
    "    <li>Create model (uses Adam optimizer for minimization).</li>\n",
    "    <li>Train model.</li>\n",
    "    <li>Hyperparameter tuning using cross-validation sets.</li>\n",
    "    <li>Retrain model until higher accuracy is achevied.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "\n",
    "The sets in the dataset are divided into five categories.\n",
    "\n",
    "\n",
    "SET A:\tZ directory with\tZ000.txt - Z100.txt<br>\n",
    "SET B: \tO directory with\tO000.txt - O100.txt<br>\n",
    "SET C:\tN directory with\tN000.txt - N100.txt<br>\n",
    "SET D:\tF directory\twith\tF000.txt - F100.txt<br>\n",
    "SET E:\tS directory with\tS000.txt - S100.txt<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_relative_path = 'dataset/random-iter-3/'\n",
    "\n",
    "datafile = dataset_relative_path + 'datafile1024.h5'\n",
    "\n",
    "with h5.File(datafile, 'r') as datafile:\n",
    "    X_train = np.array(datafile['X_train'])\n",
    "    Y_train = np.array(datafile['Y_train'])\n",
    "    \n",
    "    X_dev = np.array(datafile['X_dev'])\n",
    "    Y_dev = np.array(datafile['Y_dev'])\n",
    "    \n",
    "    X_test = np.array(datafile['X_test'])\n",
    "    Y_test = np.array(datafile['Y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dimensions_compatible(arr):\n",
    "    \n",
    "    return arr.reshape(arr.shape[0],-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = make_dimensions_compatible(X_train)\n",
    "X_dev = make_dimensions_compatible(X_dev)\n",
    "X_test = make_dimensions_compatible(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11250, 1024, 1)\n",
      "(11250, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 1000\n",
    "X_dev = X_dev / 1000\n",
    "X_test = X_test / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "WRITE TEXT HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(parameter_shapes, parameter_values = {}):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow using Xaviar's initialization.\n",
    "    The parameters are:\n",
    "    parameter_shapes: a dictionary where keys represent tensorflow variable names, and values\n",
    "    are shapes of the parameters in a list format\n",
    "    Returns:\n",
    "    params -- a dictionary of tensors containing parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    params = { }\n",
    "    \n",
    "    for n,s in parameter_shapes.items():\n",
    "        param = tf.get_variable(n, s, initializer = tf.contrib.layers.xavier_initializer())\n",
    "        params[n] = param\n",
    "    \n",
    "    for n,v in parameter_values.items():\n",
    "        params[n] = v\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "WRITE TEXT HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_cnn1(X, parameters, training=False):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    (CONV BN RELU) -> (CONV BN RELU) -> (CONV BN RELU) -> (FC RELU DROPOUT) -> FC\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters\n",
    "                  \"CONV1_W\", \"CONV2_W\", \"CONV3_W\", \"FC1_units\", \"DO_prob\", \"output_classes\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit (without softmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    CONV1_W = parameters['CONV1_W']\n",
    "    CONV1_Str = parameters['CONV1_Str']\n",
    "    CONV2_W = parameters['CONV2_W']\n",
    "    CONV2_Str = parameters['CONV2_Str']\n",
    "    CONV3_W = parameters['CONV3_W']\n",
    "    CONV3_Str = parameters['CONV3_Str']\n",
    "    FC1_units = parameters['FC1_units']\n",
    "    DO_prob = parameters['DO_prob']\n",
    "    output_classes = parameters[\"output_classes\"]\n",
    "    \n",
    "    \n",
    "    #Layer 1\n",
    "    # CONV\n",
    "    Z1 = tf.nn.conv1d(X, CONV1_W, stride=CONV1_Str, padding='VALID', data_format='NWC', name='conv1')\n",
    "    # Batch Normalization\n",
    "    B1 = tf.contrib.layers.batch_norm(Z1, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(B1)\n",
    "    \n",
    "    #Layer 2\n",
    "    # CONV\n",
    "    Z2 = tf.nn.conv1d(A1, CONV2_W, stride=CONV2_Str, padding='VALID', data_format='NWC', name='conv2')\n",
    "    # Batch Normalization\n",
    "    B2 = tf.contrib.layers.batch_norm(Z2, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(B2)\n",
    "    \n",
    "    #Layer 3\n",
    "    # CONV\n",
    "    Z3 = tf.nn.conv1d(A2, CONV3_W, stride=CONV3_Str, padding='VALID', data_format='NWC', name='conv3')\n",
    "    # Batch Normalization\n",
    "    B3 = tf.contrib.layers.batch_norm(Z3, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A3 = tf.nn.relu(B3)\n",
    "    \n",
    "    # Flatten activations for FC layer\n",
    "    A3_flat = tf.contrib.layers.flatten(A3)\n",
    "    \n",
    "    # Layer 4\n",
    "    # FC\n",
    "    A4 = tf.contrib.layers.fully_connected(A3_flat, FC1_units, activation_fn=tf.nn.relu)\n",
    "    # Dropout\n",
    "    A4_dropped = tf.contrib.layers.dropout(A4, keep_prob=DO_prob, is_training=training)\n",
    "    \n",
    "    # Layer 5\n",
    "    # FC\n",
    "    logits = tf.contrib.layers.fully_connected(A4_dropped, output_classes, activation_fn=None)\n",
    "    \n",
    "    # Although the cost function we use will have in-built softmax computations,\n",
    "    # for predictions it'll be feasible to have a named tensor\n",
    "    softmax_output = tf.nn.softmax(logits, name='softmax_output')\n",
    "    \n",
    "    return logits, softmax_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_cnn3(X, parameters, training=False):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    (CONV BN RELU) -> (CONV BN RELU DROPOUT) -> (CONV BN RELU) -> (FC RELU DROPOUT) -> FC\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters\n",
    "                  \"CONV1_W\", \"CONV2_W\", \"CONV3_W\", \"FC1_units\", \"DO_prob\", \"output_classes\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit (without softmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    CONV1_W = parameters['CONV1_W']\n",
    "    CONV1_Str = parameters['CONV1_Str']\n",
    "    CONV2_W = parameters['CONV2_W']\n",
    "    CONV2_Str = parameters['CONV2_Str']\n",
    "    DO_prob_middle_layer = parameters['DO_prob_middle_layer']\n",
    "    CONV3_W = parameters['CONV3_W']\n",
    "    CONV3_Str = parameters['CONV3_Str']\n",
    "    FC1_units = parameters['FC1_units']\n",
    "    DO_prob = parameters['DO_prob']\n",
    "    output_classes = parameters[\"output_classes\"]\n",
    "    \n",
    "    \n",
    "    #Layer 1\n",
    "    # CONV\n",
    "    Z1 = tf.nn.conv1d(X, CONV1_W, stride=CONV1_Str, padding='VALID', data_format='NWC', name='conv1')\n",
    "    # Batch Normalization\n",
    "    B1 = tf.contrib.layers.batch_norm(Z1, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(B1)\n",
    "    \n",
    "    #Layer 2\n",
    "    # CONV\n",
    "    Z2 = tf.nn.conv1d(A1, CONV2_W, stride=CONV2_Str, padding='VALID', data_format='NWC', name='conv2')\n",
    "    # Batch Normalization\n",
    "    B2 = tf.contrib.layers.batch_norm(Z2, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(B2)\n",
    "    # Dropout\n",
    "    A2_dropped = tf.contrib.layers.dropout(A2, keep_prob=DO_prob_middle_layer, is_training=training)\n",
    "    \n",
    "    #Layer 3\n",
    "    # CONV\n",
    "    Z3 = tf.nn.conv1d(A2_dropped, CONV3_W, stride=CONV3_Str, padding='VALID', data_format='NWC', name='conv3')\n",
    "    # Batch Normalization\n",
    "    B3 = tf.contrib.layers.batch_norm(Z3, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A3 = tf.nn.relu(B3)\n",
    "    \n",
    "    # Flatten activations for FC layer\n",
    "    A3_flat = tf.contrib.layers.flatten(A3)\n",
    "    \n",
    "    # Layer 4\n",
    "    # FC\n",
    "    A4 = tf.contrib.layers.fully_connected(A3_flat, FC1_units, activation_fn=tf.nn.relu)\n",
    "    # Dropout\n",
    "    A4_dropped = tf.contrib.layers.dropout(A4, keep_prob=DO_prob, is_training=training)\n",
    "    \n",
    "    # Layer 5\n",
    "    # FC\n",
    "    logits = tf.contrib.layers.fully_connected(A4_dropped, output_classes, activation_fn=None)\n",
    "    \n",
    "    # Although the cost function we use will have in-built softmax computations,\n",
    "    # for predictions it'll be feasible to have a named tensor\n",
    "    softmax_output = tf.nn.softmax(logits, name='softmax_output')\n",
    "    \n",
    "    return logits, softmax_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing cost function\n",
    "\n",
    "WRITE TEXT HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, Y, parameters, nn_key, training):\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply softmax to the output classes and find cross entropy loss\n",
    "    X - Input data\n",
    "    Y - One-hot output class training labels\n",
    "    \n",
    "    Returns:\n",
    "    cost - cross entropy loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # FIXME: setting training=training causes problems during evaluation time\n",
    "    if nn_key == 'cnn1':\n",
    "        logits, Y_hat = forward_propagation_cnn1(X, parameters, training=training)\n",
    "    elif nn_key == 'cnn2':\n",
    "        logits, Y_hat = forward_propagation_cnn1(X, parameters, training=training)\n",
    "    elif nn_key == 'cnn3':\n",
    "        logits, Y_hat = forward_propagation_cnn3(X, parameters, training=training)\n",
    "    elif nn_key == 'cnn4':\n",
    "        logits, Y_hat = forward_propagation_cnn3(X, parameters, training=training)\n",
    "    elif nn_key == 'cnn5':\n",
    "        logits, Y_hat = forward_propagation_cnn3(X, parameters, training=training)\n",
    "    elif nn_key == 'cnn6':\n",
    "        logits, Y_hat = forward_propagation_cnn1(X, parameters, training=training)\n",
    "    elif nn_key == 'cnn7':\n",
    "        logits, Y_hat = forward_propagation_cnn1(X, parameters, training=training)\n",
    "    else:\n",
    "        KeyError('Provided nn_key doesn\\'t match with any model')\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "    \n",
    "    return cost, Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisites for training\n",
    "\n",
    "Here are some procedures that are necessary to execute before the actual training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create placeholders\n",
    "\n",
    "Tensorflow functions take input in the form of `feed_dict`. The variables in other functions are placeholders for the actual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates Tensorflow placeholders that act for input data and their labels\n",
    "    \n",
    "    Arguments:\n",
    "    n_x - no. of features for X\n",
    "    n_x - no. of classes for Y\n",
    "    \n",
    "    Returns:\n",
    "    X - placeholder for data that contains input featurs,\n",
    "        shape: (no. of examples, no. of features). No. of examples is set to None\n",
    "    Y - placeholder for data that contains output class labels,\n",
    "        shape (no. of examples, no. of classes). No. of examples is set ot None\n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, name='X', shape=(None, n_x, 1))\n",
    "    Y = tf.placeholder(tf.float32, name='Y', shape=(None, n_y))\n",
    "    is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "    \n",
    "    return X,Y,is_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter shapes\n",
    "\n",
    "To initialize model parameters, we've created a procedure above. It takes as an argument a dictionary in which we supply the model parameter shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_shapes(nn_key):\n",
    "    \"\"\"\n",
    "    Get tha shapes of all parameters used in the model.\n",
    "    Convolutional layer parameter shapes (filters) are in list format\n",
    "    \n",
    "    Arguments:\n",
    "    nn_key - Provide the key for the neural network model used\n",
    "    could be, 'cnn1', 'cnn2'\n",
    "    \n",
    "    Returns:\n",
    "    param_shapes - dict that contains all the parameters as follows\n",
    "    CONV1_W, CONV2_W, CONV3_W\n",
    "    param_values:\n",
    "    CONV1_Str, CONV2_Str, CONV3_Str,\n",
    "    FC1_units, DO_prob, output_classes\n",
    "    \"\"\"\n",
    "    \n",
    "    param_shapes = {}\n",
    "    param_values = {}\n",
    "    \n",
    "    do_prob = {\n",
    "        'cnn1': 0.5,\n",
    "        'cnn2': 0.7,\n",
    "        'cnn3': 0.7,\n",
    "        'cnn4': 0.7,\n",
    "        'cnn5': 0.7,\n",
    "        'cnn6': 0.9,\n",
    "        'cnn7': 0.9\n",
    "    }\n",
    "    \n",
    "    do_prob_middle_layer = {\n",
    "        'cnn1': 0, # not used\n",
    "        'cnn2': 0, # not used\n",
    "        'cnn3': 0.2,\n",
    "        'cnn4': 0.05,\n",
    "        'cnn5': 0.01,\n",
    "        'cnn6': 0, # not used\n",
    "        'cnn7': 0 # not used\n",
    "    }\n",
    "    \n",
    "    fc1_units = {\n",
    "        'cnn1': 20,\n",
    "        'cnn2': 15,\n",
    "        'cnn3': 15,\n",
    "        'cnn4': 15,\n",
    "        'cnn5': 15,\n",
    "        'cnn6': 15,\n",
    "        'cnn7': 10\n",
    "    }\n",
    "\n",
    "    # Conv Layer 1 parameter shapes\n",
    "    # No. of channels: 24, Filter size: 5, Stride: 3\n",
    "    param_shapes['CONV1_W'] = [5, 1, 24]\n",
    "    param_values['CONV1_Str'] = 3\n",
    "    \n",
    "    # Conv Layer 2 parameter shapes\n",
    "    # No. of channels: 16, Filter size: 3, Stride: 2\n",
    "    param_shapes['CONV2_W'] = [3, 24, 16]\n",
    "    param_values['CONV2_Str'] = 2\n",
    "    \n",
    "    # Dropout after the convolutional layer 2\n",
    "    # Not used in some cases\n",
    "    param_values['DO_prob_middle_layer'] = do_prob_middle_layer[nn_key]\n",
    "    \n",
    "    # Conv Layer 3 parameter shapes\n",
    "    # No. of channels: 8, Filter size: 3, Stride: 2\n",
    "    param_shapes['CONV3_W'] = [3, 16, 8]\n",
    "    param_values['CONV3_Str'] = 2\n",
    "    \n",
    "    # Fully connected layer 1 units = 20\n",
    "    param_values['FC1_units'] = fc1_units[nn_key]\n",
    "    \n",
    "    # Dropout layer after fully connected layer 1 probability\n",
    "    param_values['DO_prob'] = do_prob[nn_key]\n",
    "    \n",
    "    # Fully connected layer 2 units (also last layer)\n",
    "    # No. of units = no. of output classes = 3\n",
    "    param_values['output_classes'] = 3\n",
    "    \n",
    "    return param_shapes, param_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random mini-batches\n",
    "\n",
    "For each epoch we'll use different sets of mini-batches to avoid any possible overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (number of examples, window size) (m, n_x)\n",
    "    Y -- output classes, of shape (number of examples, output classes) (m, n_y)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = np.floor(m/mini_batch_size).astype(int) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting costs\n",
    "\n",
    "WRITE TEXT HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_costs(costs, dev_costs, learning_rate, total_epochs):\n",
    "    # plot the cost\n",
    "    plt.plot(costs, color='blue', label='training')\n",
    "    plt.plot(dev_costs, color='green', label='dev')\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.title(\"Learning rate = %f\\nTotal Epochs = %i\" % (learning_rate, total_epochs))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data\n",
    "\n",
    "WRITE TEXT HERE...\n",
    "\n",
    "[UPDATE_OPS](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_dev, Y_dev,\n",
    "          learning_rate = 0.009, num_epochs = 100, minibatch_size = 64, print_cost = True,\n",
    "          save_session_path=None, model_file=None, restore_session=False, save_session_interval=5, max_to_keep=10,\n",
    "          nn_key='cnn1'):\n",
    "    \"\"\"\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_train -- test set, of shape (None, n_y = 6)\n",
    "    X_test -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_test -- test set, of shape (None, n_y = 6)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    (m, n_x,_) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    dev_costs = []\n",
    "    \n",
    "    model_path = None\n",
    "    if (save_session_path != None and model_file != None):\n",
    "        model_path = save_session_path + model_file\n",
    "    \n",
    "    \n",
    "    # Create Placeholders of the correct shape\n",
    "    X, Y, is_train = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    param_shapes, param_values = parameter_shapes(nn_key)\n",
    "    parameters = initialize_parameters(param_shapes, param_values)\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    # Prediction: Use Y_hat to compute the output class during prediction\n",
    "    cost, Y_hat = compute_cost(X, Y, parameters, nn_key, is_train)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    # optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # For saving / restoring sesison when training for long\n",
    "    epoch_counter = tf.get_variable('epoch_counter', shape=[], initializer=tf.zeros_initializer)\n",
    "    counter_op = tf.assign_add(epoch_counter, 1)\n",
    "    saver = tf.train.Saver(max_to_keep=max_to_keep)\n",
    "    \n",
    "    # Calculate the correct predictions\n",
    "    predict_op = tf.argmax(Y_hat, 1)\n",
    "    correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "\n",
    "    # Calculate accuracy on the test set\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    # For impementation of batch norm the tf.GraphKeys.UPDATE_OPS dependency needs to be added\n",
    "    # see documentation on tf.contrib.layers.batch_norm\n",
    "#     update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess: #, tf.control_dependencies(update_ops):\n",
    "        \n",
    "        if (restore_session == False and path.exists(save_session_path)):\n",
    "            raise FileExistsError('Session already exists, either restore the session, or manually delete the files.')\n",
    "        \n",
    "        # restore the previous session if the path already exists\n",
    "        if (model_path != None and restore_session==True):\n",
    "            print(\"Restoring session...\\n\")\n",
    "            saver.restore(sess, model_path)\n",
    "            print(\"Previous epoch counter: %i\\n\\n\" % epoch_counter.eval())\n",
    "        else:\n",
    "            sess.run(init)\n",
    "        \n",
    "        tf.train.export_meta_graph(model_path + '.meta') # save the model file (.meta) only once\n",
    "        \n",
    "        print(\"Cost at start: %f\" % cost.eval({X: X_train, Y: Y_train, is_train: False}))\n",
    "        print(\"Dev cost: %f\" % cost.eval({X: X_dev, Y: Y_dev, is_train: False}))\n",
    "        \n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train, is_train: False})\n",
    "        dev_accuracy = accuracy.eval({X: X_dev, Y: Y_dev, is_train: False})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Dev Accuracy:\", dev_accuracy)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                \n",
    "                try:\n",
    "\n",
    "                    # Select a minibatch\n",
    "                    (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                    # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                    # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                    _,minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y, is_train: True})\n",
    "\n",
    "                    epoch_cost += minibatch_cost / num_minibatches\n",
    "                \n",
    "                # Implement early stopping mechanism on KeyboardInterrupt\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"KeyboardInterrupt received. Stopping early\")\n",
    "                    plot_costs(np.squeeze(costs), np.squeeze(dev_costs), learning_rate, epoch_counter.eval())\n",
    "                    return parameters\n",
    "                \n",
    "            \n",
    "            if (epoch % save_session_interval == 0 and save_session_path != None):\n",
    "                saver.save(sess, model_path, write_meta_graph=False)\n",
    "            \n",
    "            # Save the costs after each epoch for plotting learning curve\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                dev_cost = cost.eval({X: X_dev, Y: Y_dev, is_train: False})\n",
    "                dev_costs.append(dev_cost)\n",
    "                \n",
    "                \n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and (epoch + 1) % 5 == 0:\n",
    "                print (\"\\nCost after epoch %i: %f\" % (epoch + 1, epoch_cost))\n",
    "                print (\"Dev cost after epoch %i: %f\" % (epoch + 1, dev_cost))\n",
    "                \n",
    "                train_accuracy = accuracy.eval({X: X_train, Y: Y_train, is_train: False})\n",
    "                dev_accuracy = accuracy.eval({X: X_dev, Y: Y_dev, is_train: False})\n",
    "                print(\"Train Accuracy:\", train_accuracy)\n",
    "                print(\"Dev Accuracy:\", dev_accuracy)\n",
    "            \n",
    "            # increment the epoch_counter in case the session is saved\n",
    "            # and restored later\n",
    "            sess.run(counter_op)\n",
    "                \n",
    "                \n",
    "        if (save_session_path != None):\n",
    "            saver.save(sess, model_path, write_meta_graph=False)\n",
    "        \n",
    "        \n",
    "        plot_costs(np.squeeze(costs), np.squeeze(dev_costs), learning_rate, epoch_counter.eval())\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train, is_train: False})\n",
    "        dev_accuracy = accuracy.eval({X: X_dev, Y: Y_dev, is_train: False})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Dev Accuracy:\", dev_accuracy)\n",
    "                \n",
    "        return parameters\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring session...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from train/dataset-1024-3/cnn7_lr-0.0005_mbs-128/model\n",
      "Previous epoch counter: 370\n",
      "\n",
      "\n",
      "Cost at start: 0.005425\n",
      "Dev cost: 0.804326\n",
      "Train Accuracy: 0.99777776\n",
      "Dev Accuracy: 0.92\n",
      "\n",
      "Cost after epoch 5: 0.002111\n",
      "Dev cost after epoch 5: 0.792241\n",
      "Train Accuracy: 0.99902225\n",
      "Dev Accuracy: 0.92\n",
      "\n",
      "Cost after epoch 10: 0.001941\n",
      "Dev cost after epoch 10: 0.780104\n",
      "Train Accuracy: 0.9998222\n",
      "Dev Accuracy: 0.92\n",
      "\n",
      "Cost after epoch 15: 0.001556\n",
      "Dev cost after epoch 15: 0.762383\n",
      "Train Accuracy: 0.9999111\n",
      "Dev Accuracy: 0.92\n",
      "\n",
      "Cost after epoch 20: 0.001724\n",
      "Dev cost after epoch 20: 0.750109\n",
      "Train Accuracy: 0.9999111\n",
      "Dev Accuracy: 0.92\n",
      "\n",
      "Cost after epoch 25: 0.002185\n",
      "Dev cost after epoch 25: 0.729247\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.9257143\n",
      "\n",
      "Cost after epoch 30: 0.002130\n",
      "Dev cost after epoch 30: 0.711144\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.9257143\n",
      "\n",
      "Cost after epoch 35: 0.001998\n",
      "Dev cost after epoch 35: 0.696494\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.9257143\n",
      "\n",
      "Cost after epoch 40: 0.001734\n",
      "Dev cost after epoch 40: 0.681234\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.9257143\n",
      "\n",
      "Cost after epoch 45: 0.002225\n",
      "Dev cost after epoch 45: 0.672240\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.9257143\n",
      "\n",
      "Cost after epoch 50: 0.001692\n",
      "Dev cost after epoch 50: 0.666045\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93142855\n",
      "\n",
      "Cost after epoch 55: 0.001425\n",
      "Dev cost after epoch 55: 0.661259\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93142855\n",
      "\n",
      "Cost after epoch 60: 0.002405\n",
      "Dev cost after epoch 60: 0.652548\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93142855\n",
      "\n",
      "Cost after epoch 65: 0.002109\n",
      "Dev cost after epoch 65: 0.643834\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93142855\n",
      "\n",
      "Cost after epoch 70: 0.001590\n",
      "Dev cost after epoch 70: 0.634496\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93142855\n",
      "\n",
      "Cost after epoch 75: 0.001808\n",
      "Dev cost after epoch 75: 0.635128\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93142855\n",
      "\n",
      "Cost after epoch 80: 0.001315\n",
      "Dev cost after epoch 80: 0.629049\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93142855\n",
      "\n",
      "Cost after epoch 85: 0.001129\n",
      "Dev cost after epoch 85: 0.629396\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93142855\n",
      "\n",
      "Cost after epoch 90: 0.002193\n",
      "Dev cost after epoch 90: 0.629258\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93142855\n",
      "\n",
      "Cost after epoch 95: 0.001397\n",
      "Dev cost after epoch 95: 0.626493\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 100: 0.001917\n",
      "Dev cost after epoch 100: 0.618743\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 105: 0.001070\n",
      "Dev cost after epoch 105: 0.618358\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 110: 0.001834\n",
      "Dev cost after epoch 110: 0.608502\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 115: 0.001851\n",
      "Dev cost after epoch 115: 0.602120\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 120: 0.001495\n",
      "Dev cost after epoch 120: 0.604778\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 125: 0.001594\n",
      "Dev cost after epoch 125: 0.604657\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 130: 0.001508\n",
      "Dev cost after epoch 130: 0.594891\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 135: 0.001748\n",
      "Dev cost after epoch 135: 0.583539\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 140: 0.001423\n",
      "Dev cost after epoch 140: 0.586105\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 145: 0.002384\n",
      "Dev cost after epoch 145: 0.602353\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 150: 0.001702\n",
      "Dev cost after epoch 150: 0.601934\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 155: 0.001988\n",
      "Dev cost after epoch 155: 0.604772\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 160: 0.002288\n",
      "Dev cost after epoch 160: 0.603707\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 165: 0.002172\n",
      "Dev cost after epoch 165: 0.602422\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 170: 0.001928\n",
      "Dev cost after epoch 170: 0.598622\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 175: 0.002619\n",
      "Dev cost after epoch 175: 0.603641\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 180: 0.001285\n",
      "Dev cost after epoch 180: 0.606787\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 185: 0.001079\n",
      "Dev cost after epoch 185: 0.618757\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 190: 0.001594\n",
      "Dev cost after epoch 190: 0.614727\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 195: 0.001853\n",
      "Dev cost after epoch 195: 0.601816\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 200: 0.001645\n",
      "Dev cost after epoch 200: 0.609832\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 205: 0.001353\n",
      "Dev cost after epoch 205: 0.622685\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 210: 0.001699\n",
      "Dev cost after epoch 210: 0.623080\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 215: 0.001126\n",
      "Dev cost after epoch 215: 0.630205\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 220: 0.001532\n",
      "Dev cost after epoch 220: 0.621170\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 225: 0.001424\n",
      "Dev cost after epoch 225: 0.603912\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 230: 0.001703\n",
      "Dev cost after epoch 230: 0.592898\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 235: 0.002032\n",
      "Dev cost after epoch 235: 0.597405\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 240: 0.002366\n",
      "Dev cost after epoch 240: 0.583580\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 245: 0.002124\n",
      "Dev cost after epoch 245: 0.611872\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 250: 0.001328\n",
      "Dev cost after epoch 250: 0.607971\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 255: 0.001321\n",
      "Dev cost after epoch 255: 0.605752\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 260: 0.001622\n",
      "Dev cost after epoch 260: 0.613424\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 265: 0.001221\n",
      "Dev cost after epoch 265: 0.619892\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 270: 0.001244\n",
      "Dev cost after epoch 270: 0.623279\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 275: 0.001731\n",
      "Dev cost after epoch 275: 0.615744\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 280: 0.001505\n",
      "Dev cost after epoch 280: 0.612097\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 285: 0.002100\n",
      "Dev cost after epoch 285: 0.656880\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 290: 0.001270\n",
      "Dev cost after epoch 290: 0.664494\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 295: 0.000901\n",
      "Dev cost after epoch 295: 0.686341\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 300: 0.001694\n",
      "Dev cost after epoch 300: 0.677102\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "\n",
      "Cost after epoch 305: 0.001362\n",
      "Dev cost after epoch 305: 0.662517\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 0.93714285\n",
      "KeyboardInterrupt received. Stopping early\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_dev, Y_dev,\n",
    "                   learning_rate=0.00001,\n",
    "                   num_epochs=1000,\n",
    "                   minibatch_size=512,\n",
    "                   save_session_path='train/dataset-1024-3/cnn7_lr-0.0005_mbs-128/',\n",
    "                   model_file='model',\n",
    "                   restore_session=True,\n",
    "                   save_session_interval=10,\n",
    "                   nn_key='cnn7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and restoring saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, session_path, model_file, Y_test_onehot=None):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    checkpoint_path = session_path\n",
    "    model_path = session_path + model_file\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        loader = tf.train.import_meta_graph(model_path)\n",
    "        loader.restore(sess, tf.train.latest_checkpoint(checkpoint_path))\n",
    "\n",
    "        graph = tf.get_default_graph()\n",
    "\n",
    "        X = graph.get_tensor_by_name('X:0')\n",
    "        Y = graph.get_tensor_by_name('Y:0')\n",
    "        is_train = graph.get_tensor_by_name('is_train:0')\n",
    "\n",
    "        Y_hat = graph.get_tensor_by_name('softmax_output:0')\n",
    "\n",
    "        predict_op = tf.argmax(Y_hat, 1)\n",
    "\n",
    "        y_hat_test = predict_op.eval({X: X_test, is_train: False})\n",
    "        \n",
    "        # print the accuracy of the test set if the labels are provided\n",
    "        if (Y_test_onehot is not None):\n",
    "            y_test = np.argmax(Y_test_onehot, 1)\n",
    "            print('Accuracy: %f' % (y_hat_test == y_test).mean())\n",
    "        \n",
    "\n",
    "    return y_hat_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_voting(X_test_voting, session_path, model_file):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    checkpoint_path = session_path\n",
    "    model_path = session_path + model_file\n",
    "    \n",
    "    y_hat_test_voting = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        loader = tf.train.import_meta_graph(model_path)\n",
    "        loader.restore(sess, tf.train.latest_checkpoint(checkpoint_path))\n",
    "\n",
    "        graph = tf.get_default_graph()\n",
    "\n",
    "        X = graph.get_tensor_by_name('X:0')\n",
    "        is_train = graph.get_tensor_by_name('is_train:0')\n",
    "\n",
    "        Y_hat = graph.get_tensor_by_name('softmax_output:0')\n",
    "\n",
    "        predict_op = tf.argmax(Y_hat, 1)\n",
    "        \n",
    "        classname, idx, counts = tf.unique_with_counts(predict_op)\n",
    "        predict_voting_op = tf.gather(classname, tf.argmax(counts))\n",
    "\n",
    "        # no. of training examples with the original feature size\n",
    "        m = X_test_voting.shape[0]\n",
    "        \n",
    "        # no. of split training examples of each original example\n",
    "        m_each = X_test_voting.shape[1]\n",
    "        \n",
    "        for ex in range(m):\n",
    "            x_test_voting = make_dimensions_compatible(X_test_voting[ex])\n",
    "            pred = predict_voting_op.eval({X: x_test_voting, is_train: False})\n",
    "            \n",
    "            y_hat_test_voting.append(pred)\n",
    "\n",
    "    return y_hat_test_voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from train/dataset-1024-3/cnn7_lr-0.0005_mbs-128/model\n",
      "Accuracy: 0.937143\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 2 2 2 2 1 2\n",
      " 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(X_dev, 'train/dataset-1024-3/cnn7_lr-0.0005_mbs-128/', 'model.meta', Y_test_onehot=Y_dev)\n",
    "\n",
    "print(predictions)\n",
    "print(np.argmax(Y_test, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read test data for voting accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from train/dataset-1024-3/cnn7_lr-0.0005_mbs-128/model\n",
      "Accuracy with voting: 1.000000\n"
     ]
    }
   ],
   "source": [
    "testfile = dataset_relative_path + 'testset_voting_1024.h5'\n",
    "session_path = 'train/dataset-1024-3/cnn7_lr-0.0005_mbs-128/'\n",
    "model_file = 'model.meta'\n",
    "\n",
    "with h5.File(testfile, 'r') as testfile:\n",
    "    X_test_voting = testfile['X']\n",
    "    X_test_voting = np.array(X_test_voting) / 1000\n",
    "    y_test_voting = np.array(testfile['Y'])\n",
    "    \n",
    "    y_hat_test_voting = predict_voting(X_test_voting, session_path, model_file)\n",
    "    \n",
    "    print(\"Accuracy with voting: %f\" % (y_test_voting == y_hat_test_voting).mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat_test_voting)\n",
    "print(y_test_voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(Y_dev, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
