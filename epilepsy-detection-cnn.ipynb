{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing modules\n",
    "\n",
    "Let's first add these libraries to our project:\n",
    "\n",
    "`numpy`: for matrix operations\n",
    "\n",
    "`tensorlfow`: deep learning layers\n",
    "\n",
    "`maplotlitb`: visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py as h5'\n",
    "from tensorflow.python.framework import ops\n",
    "from os import path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for solving the problem\n",
    "\n",
    "<ol>\n",
    "    <li>Read data and format it.</li>\n",
    "    <li>Use sliding window approach to augment data.</li>\n",
    "    <li>Split data into training/dev/test sets.</li>\n",
    "    <li>Create procedure for randomly initializing parameters with specified shape using Xavier's initialization.</li>\n",
    "    <li>Create convolution and pooling procedures.</li>\n",
    "    <li>Implement forward propagation.</li>\n",
    "    <li>Implement cost function.</li>\n",
    "    <li>Create model (uses Adam optimizer for minimization).</li>\n",
    "    <li>Train model.</li>\n",
    "    <li>Hyperparameter tuning using cross-validation sets.</li>\n",
    "    <li>Retrain model until higher accuracy is achevied.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "\n",
    "Use `dataset_relative_path` to point to the directory where the dataset after processing has been stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_relative_path = 'dataset/random-iter-5/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = dataset_relative_path + 'datafile1024.h5'\n",
    "\n",
    "with h5.File(datafile, 'r') as datafile:\n",
    "    X_train = np.array(datafile['X_train'])\n",
    "    Y_train = np.array(datafile['Y_train'])\n",
    "    \n",
    "    X_dev = np.array(datafile['X_dev'])\n",
    "    Y_dev = np.array(datafile['Y_dev'])\n",
    "    \n",
    "    X_test = np.array(datafile['X_test'])\n",
    "    Y_test = np.array(datafile['Y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dimensions_compatible(arr):\n",
    "    \n",
    "    return arr.reshape(arr.shape[0],-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = make_dimensions_compatible(X_train)\n",
    "X_dev = make_dimensions_compatible(X_dev)\n",
    "X_test = make_dimensions_compatible(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11250, 1024, 1)\n",
      "(11250, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "It is a standard procedure to use normalization formula that subtracts by mean and divides by standard deviation. However, for the purpose of simplicity it won't hurt the performance of the models too much to just divide by 1000 since most of the data points are voltage measures with values ranging significantly within -1000 and 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 1000\n",
    "X_dev = X_dev / 1000\n",
    "X_test = X_test / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Deep neural networks suffer from a problem of exploding or vanishing gradients. To reduce the effect, we use Xavier's initialization which is already built into the Tensorflow.\n",
    "\n",
    "`intiialize_parameters` receives the shapes and values of different parameters and hyper parameters to be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(parameter_shapes, parameter_values = {}):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow using Xaviar's initialization.\n",
    "    The parameters are:\n",
    "    parameter_shapes: a dictionary where keys represent tensorflow variable names, and values\n",
    "    are shapes of the parameters in a list format\n",
    "    Returns:\n",
    "    params -- a dictionary of tensors containing parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    params = { }\n",
    "    \n",
    "    for n,s in parameter_shapes.items():\n",
    "        param = tf.get_variable(n, s, initializer = tf.contrib.layers.xavier_initializer())\n",
    "        params[n] = param\n",
    "    \n",
    "    for n,v in parameter_values.items():\n",
    "        params[n] = v\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "Forward propagation builds most of the computation graph of the models and defines the layers for each model.\n",
    "\n",
    "For models with different set of layers or different architecture, we define different forward propagation function. Models having similar architecture and which only differ by parameter shapes or hyper parameters share common function.\n",
    "\n",
    "The architectures of the models we trained are described in the literature of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_cnn1(X, parameters, training=False):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    (CONV BN RELU) -> (CONV BN RELU) -> (CONV BN RELU) -> (FC RELU DROPOUT) -> FC\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters\n",
    "                  \"CONV1_W\", \"CONV2_W\", \"CONV3_W\", \"FC1_units\", \"DO_prob\", \"output_classes\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit (without softmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    CONV1_W = parameters['CONV1_W']\n",
    "    CONV1_Str = parameters['CONV1_Str']\n",
    "    CONV2_W = parameters['CONV2_W']\n",
    "    CONV2_Str = parameters['CONV2_Str']\n",
    "    CONV3_W = parameters['CONV3_W']\n",
    "    CONV3_Str = parameters['CONV3_Str']\n",
    "    FC1_units = parameters['FC1_units']\n",
    "    DO_prob = parameters['DO_prob']\n",
    "    output_classes = parameters[\"output_classes\"]\n",
    "    \n",
    "    \n",
    "    #Layer 1\n",
    "    # CONV\n",
    "    Z1 = tf.nn.conv1d(X, CONV1_W, stride=CONV1_Str, padding='VALID', data_format='NWC', name='conv1')\n",
    "    # Batch Normalization\n",
    "    B1 = tf.contrib.layers.batch_norm(Z1, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(B1)\n",
    "    \n",
    "    #Layer 2\n",
    "    # CONV\n",
    "    Z2 = tf.nn.conv1d(A1, CONV2_W, stride=CONV2_Str, padding='VALID', data_format='NWC', name='conv2')\n",
    "    # Batch Normalization\n",
    "    B2 = tf.contrib.layers.batch_norm(Z2, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(B2)\n",
    "    \n",
    "    #Layer 3\n",
    "    # CONV\n",
    "    Z3 = tf.nn.conv1d(A2, CONV3_W, stride=CONV3_Str, padding='VALID', data_format='NWC', name='conv3')\n",
    "    # Batch Normalization\n",
    "    B3 = tf.contrib.layers.batch_norm(Z3, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A3 = tf.nn.relu(B3)\n",
    "    \n",
    "    # Flatten activations for FC layer\n",
    "    A3_flat = tf.contrib.layers.flatten(A3)\n",
    "    \n",
    "    # Layer 4\n",
    "    # FC\n",
    "    A4 = tf.contrib.layers.fully_connected(A3_flat, FC1_units, activation_fn=tf.nn.relu)\n",
    "    # Dropout\n",
    "    A4_dropped = tf.contrib.layers.dropout(A4, keep_prob=DO_prob, is_training=training)\n",
    "    \n",
    "    # Layer 5\n",
    "    # FC\n",
    "    logits = tf.contrib.layers.fully_connected(A4_dropped, output_classes, activation_fn=None)\n",
    "    \n",
    "    # Although the cost function we use will have in-built softmax computations,\n",
    "    # for predictions it'll be feasible to have a named tensor\n",
    "    softmax_output = tf.nn.softmax(logits, name='softmax_output')\n",
    "    \n",
    "    return logits, softmax_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_cnn3(X, parameters, training=False):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    (CONV BN RELU) -> (CONV BN RELU DROPOUT) -> (CONV BN RELU) -> (FC RELU DROPOUT) -> FC\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters\n",
    "                  \"CONV1_W\", \"CONV2_W\", \"CONV3_W\", \"FC1_units\", \"DO_prob\", \"output_classes\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit (without softmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    CONV1_W = parameters['CONV1_W']\n",
    "    CONV1_Str = parameters['CONV1_Str']\n",
    "    CONV2_W = parameters['CONV2_W']\n",
    "    CONV2_Str = parameters['CONV2_Str']\n",
    "    DO_prob_middle_layer = parameters['DO_prob_middle_layer']\n",
    "    CONV3_W = parameters['CONV3_W']\n",
    "    CONV3_Str = parameters['CONV3_Str']\n",
    "    FC1_units = parameters['FC1_units']\n",
    "    DO_prob = parameters['DO_prob']\n",
    "    output_classes = parameters[\"output_classes\"]\n",
    "    \n",
    "    \n",
    "    #Layer 1\n",
    "    # CONV\n",
    "    Z1 = tf.nn.conv1d(X, CONV1_W, stride=CONV1_Str, padding='VALID', data_format='NWC', name='conv1')\n",
    "    # Batch Normalization\n",
    "    B1 = tf.contrib.layers.batch_norm(Z1, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(B1)\n",
    "    \n",
    "    #Layer 2\n",
    "    # CONV\n",
    "    Z2 = tf.nn.conv1d(A1, CONV2_W, stride=CONV2_Str, padding='VALID', data_format='NWC', name='conv2')\n",
    "    # Batch Normalization\n",
    "    B2 = tf.contrib.layers.batch_norm(Z2, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(B2)\n",
    "    # Dropout\n",
    "    A2_dropped = tf.contrib.layers.dropout(A2, keep_prob=DO_prob_middle_layer, is_training=training)\n",
    "    \n",
    "    #Layer 3\n",
    "    # CONV\n",
    "    Z3 = tf.nn.conv1d(A2_dropped, CONV3_W, stride=CONV3_Str, padding='VALID', data_format='NWC', name='conv3')\n",
    "    # Batch Normalization\n",
    "    B3 = tf.contrib.layers.batch_norm(Z3, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A3 = tf.nn.relu(B3)\n",
    "    \n",
    "    # Flatten activations for FC layer\n",
    "    A3_flat = tf.contrib.layers.flatten(A3)\n",
    "    \n",
    "    # Layer 4\n",
    "    # FC\n",
    "    A4 = tf.contrib.layers.fully_connected(A3_flat, FC1_units, activation_fn=tf.nn.relu)\n",
    "    # Dropout\n",
    "    A4_dropped = tf.contrib.layers.dropout(A4, keep_prob=DO_prob, is_training=training)\n",
    "    \n",
    "    # Layer 5\n",
    "    # FC\n",
    "    logits = tf.contrib.layers.fully_connected(A4_dropped, output_classes, activation_fn=None)\n",
    "    \n",
    "    # Although the cost function we use will have in-built softmax computations,\n",
    "    # for predictions it'll be feasible to have a named tensor\n",
    "    softmax_output = tf.nn.softmax(logits, name='softmax_output')\n",
    "    \n",
    "    return logits, softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_cnn8(X, parameters, training=False):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    (CONV BN RELU) -> (CONV BN RELU) -> (FC RELU DROPOUT) -> FC\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters\n",
    "                  \"CONV1_W\", \"CONV2_W\", \"CONV3_W\", \"FC1_units\", \"DO_prob\", \"output_classes\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit (without softmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    CONV1_W = parameters['CONV1_W']\n",
    "    CONV1_Str = parameters['CONV1_Str']\n",
    "    CONV2_W = parameters['CONV2_W']\n",
    "    CONV2_Str = parameters['CONV2_Str']\n",
    "    DO_prob_middle_layer = parameters['DO_prob_middle_layer']\n",
    "    FC1_units = parameters['FC1_units']\n",
    "    DO_prob = parameters['DO_prob']\n",
    "    output_classes = parameters[\"output_classes\"]\n",
    "    \n",
    "    \n",
    "    #Layer 1\n",
    "    # CONV\n",
    "    Z1 = tf.nn.conv1d(X, CONV1_W, stride=CONV1_Str, padding='VALID', data_format='NWC', name='conv1')\n",
    "    # Batch Normalization\n",
    "    B1 = tf.contrib.layers.batch_norm(Z1, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(B1)\n",
    "    \n",
    "    #Layer 2\n",
    "    # CONV\n",
    "    Z2 = tf.nn.conv1d(A1, CONV2_W, stride=CONV2_Str, padding='VALID', data_format='NWC', name='conv2')\n",
    "    # Batch Normalization\n",
    "    B2 = tf.contrib.layers.batch_norm(Z2, is_training=training, updates_collections=None)\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(B2)\n",
    "    \n",
    "    # Flatten activations for FC layer\n",
    "    A2_flat = tf.contrib.layers.flatten(A2)\n",
    "    \n",
    "    # Layer 3\n",
    "    # FC\n",
    "    A3 = tf.contrib.layers.fully_connected(A2_flat, FC1_units, activation_fn=tf.nn.relu)\n",
    "    # Dropout\n",
    "    A3_dropped = tf.contrib.layers.dropout(A3, keep_prob=DO_prob, is_training=training)\n",
    "    \n",
    "    # Layer 4\n",
    "    # FC\n",
    "    logits = tf.contrib.layers.fully_connected(A3_dropped, output_classes, activation_fn=None)\n",
    "    \n",
    "    # Although the cost function we use will have in-built softmax computations,\n",
    "    # for predictions it'll be feasible to have a named tensor\n",
    "    softmax_output = tf.nn.softmax(logits, name='softmax_output')\n",
    "    \n",
    "    return logits, softmax_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing cost function\n",
    "\n",
    "We use cross entropy loss for our classification problem which takes logits from forward propagation as one of its input. The softmax layer's output from forward propagation functions defined above is not used for computing cost and is used for making predictions at the end of this notebook. The cost function of cross entropy which is built in the Tensorflow computes its own softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, Y, parameters, nn_key, training):\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply softmax to the output classes and find cross entropy loss\n",
    "    X - Input data\n",
    "    Y - One-hot output class training labels\n",
    "    \n",
    "    Returns:\n",
    "    cost - cross entropy loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # FIXME: setting training=training causes problems during evaluation time\n",
    "    if nn_key == 'cnn1':\n",
    "        logits, Y_hat = forward_propagation_cnn1(X, parameters, training=training)\n",
    "    elif nn_key == 'cnn2':\n",
    "        logits, Y_hat = forward_propagation_cnn1(X, parameters, training=training)\n",
    "    elif nn_key == 'cnn3':\n",
    "        logits, Y_hat = forward_propagation_cnn3(X, parameters, training=training)\n",
    "    elif nn_key == 'cnn4':\n",
    "        logits, Y_hat = forward_propagation_cnn1(X, parameters, training=training)\n",
    "    elif nn_key == 'cnn5':\n",
    "        logits, Y_hat = forward_propagation_cnn8(X, parameters, training=training)\n",
    "    elif nn_key == 'cnn6':\n",
    "        logits, Y_hat = forward_propagation_cnn8(X, parameters, training=training)\n",
    "    elif nn_key == 'cnn7':\n",
    "        logits, Y_hat = forward_propagation_cnn8(X, parameters, training=training)\n",
    "    elif nn_key == 'cnn8':\n",
    "        logits, Y_hat = forward_propagation_cnn8(X, parameters, training=training)\n",
    "    else:\n",
    "        KeyError('Provided nn_key doesn\\'t match with any model')\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "    \n",
    "    return cost, Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisites for training\n",
    "\n",
    "Here are some procedures that are necessary to execute before the actual training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create placeholders\n",
    "\n",
    "Tensorflow functions take input in the form of `feed_dict`. The variables in other functions are placeholders for the actual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates Tensorflow placeholders that act for input data and their labels\n",
    "    \n",
    "    Arguments:\n",
    "    n_x - no. of features for X\n",
    "    n_x - no. of classes for Y\n",
    "    \n",
    "    Returns:\n",
    "    X - placeholder for data that contains input featurs,\n",
    "        shape: (no. of examples, no. of features). No. of examples is set to None\n",
    "    Y - placeholder for data that contains output class labels,\n",
    "        shape (no. of examples, no. of classes). No. of examples is set ot None\n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, name='X', shape=(None, n_x, 1))\n",
    "    Y = tf.placeholder(tf.float32, name='Y', shape=(None, n_y))\n",
    "    is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "    \n",
    "    return X,Y,is_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter shapes\n",
    "\n",
    "To initialize model parameters, we've created a procedure above. It takes as an argument a dictionary in which we supply the model parameter shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_shapes(nn_key):\n",
    "    \"\"\"\n",
    "    Get tha shapes of all parameters used in the model.\n",
    "    Convolutional layer parameter shapes (filters) are in list format\n",
    "    \n",
    "    Arguments:\n",
    "    nn_key - Provide the key for the neural network model used\n",
    "    could be, 'cnn1', 'cnn2'\n",
    "    \n",
    "    Returns:\n",
    "    param_shapes - dict that contains all the parameters as follows\n",
    "    CONV1_W, CONV2_W, CONV3_W\n",
    "    param_values:\n",
    "    CONV1_Str, CONV2_Str, CONV3_Str,\n",
    "    FC1_units, DO_prob, output_classes\n",
    "    \"\"\"\n",
    "    \n",
    "    param_shapes = {}\n",
    "    param_values = {}\n",
    "    \n",
    "    do_prob = {\n",
    "        'cnn1': 0.5,\n",
    "        'cnn2': 0.3,\n",
    "        'cnn3': 0.3,\n",
    "        'cnn4': 0.9,\n",
    "        'cnn5': 0.5,\n",
    "        'cnn6': 0.7,\n",
    "        'cnn7': 0.3,\n",
    "        'cnn8': 0.3\n",
    "    }\n",
    "    \n",
    "    do_prob_middle_layer = {\n",
    "        'cnn1': 0,   # not used\n",
    "        'cnn2': 0,   # not used\n",
    "        'cnn3': 0.8,\n",
    "        'cnn4': 0,   # not used\n",
    "        'cnn5': 0,   # not used\n",
    "        'cnn6': 0,   # not used\n",
    "        'cnn7': 0,   # not used\n",
    "        'cnn8': 0    # not used\n",
    "    }\n",
    "    \n",
    "    fc1_units = {\n",
    "        'cnn1': 20,\n",
    "        'cnn2': 15,\n",
    "        'cnn3': 15,\n",
    "        'cnn4': 15,\n",
    "        'cnn5': 20,\n",
    "        'cnn6': 15,\n",
    "        'cnn7': 15,\n",
    "        'cnn8': 10\n",
    "    }\n",
    "\n",
    "    # Conv Layer 1 parameter shapes\n",
    "    # No. of channels: 24, Filter size: 5, Stride: 3\n",
    "    param_shapes['CONV1_W'] = [5, 1, 24]\n",
    "    param_values['CONV1_Str'] = 3\n",
    "    \n",
    "    # Conv Layer 2 parameter shapes\n",
    "    # No. of channels: 16, Filter size: 3, Stride: 2\n",
    "    param_shapes['CONV2_W'] = [3, 24, 16]\n",
    "    param_values['CONV2_Str'] = 2\n",
    "    \n",
    "    # Dropout after the convolutional layer 2\n",
    "    # Not used in some cases\n",
    "    param_values['DO_prob_middle_layer'] = do_prob_middle_layer[nn_key]\n",
    "    \n",
    "    # Conv Layer 3 parameter shapes\n",
    "    # No. of channels: 8, Filter size: 3, Stride: 2\n",
    "    param_shapes['CONV3_W'] = [3, 16, 8]\n",
    "    param_values['CONV3_Str'] = 2\n",
    "    \n",
    "    # Fully connected layer 1 units = 20\n",
    "    param_values['FC1_units'] = fc1_units[nn_key]\n",
    "    \n",
    "    # Dropout layer after fully connected layer 1 probability\n",
    "    param_values['DO_prob'] = do_prob[nn_key]\n",
    "    \n",
    "    # Fully connected layer 2 units (also last layer)\n",
    "    # No. of units = no. of output classes = 3\n",
    "    param_values['output_classes'] = 3\n",
    "    \n",
    "    return param_shapes, param_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random mini-batches\n",
    "\n",
    "For each epoch we'll use different sets of mini-batches to avoid any possible overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (number of examples, window size) (m, n_x)\n",
    "    Y -- output classes, of shape (number of examples, output classes) (m, n_y)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = np.floor(m/mini_batch_size).astype(int) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting costs\n",
    "\n",
    "At the end of each training session, we'll plot the learning curves for training and dev sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_costs(costs, dev_costs, learning_rate, total_epochs):\n",
    "    # plot the cost\n",
    "    plt.plot(costs, color='blue', label='training')\n",
    "    plt.plot(dev_costs, color='green', label='dev')\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.title(\"Learning rate = %f\\nTotal Epochs = %i\" % (learning_rate, total_epochs))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data\n",
    "\n",
    "WRITE TEXT HERE...\n",
    "\n",
    "[UPDATE_OPS](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_dev, Y_dev,\n",
    "          learning_rate = 0.009, num_epochs = 100, minibatch_size = 64, print_cost = True,\n",
    "          save_session_path=None, model_file=None, restore_session=False, save_session_interval=5, max_to_keep=10,\n",
    "          nn_key='cnn1'):\n",
    "    \"\"\"\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_train -- test set, of shape (None, n_y = 6)\n",
    "    X_test -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_test -- test set, of shape (None, n_y = 6)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    (m, n_x,_) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    dev_costs = []\n",
    "    \n",
    "    model_path = None\n",
    "    if (save_session_path != None and model_file != None):\n",
    "        model_path = save_session_path + model_file\n",
    "    \n",
    "    \n",
    "    # Create Placeholders of the correct shape\n",
    "    X, Y, is_train = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    param_shapes, param_values = parameter_shapes(nn_key)\n",
    "    parameters = initialize_parameters(param_shapes, param_values)\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    # Prediction: Use Y_hat to compute the output class during prediction\n",
    "    cost, Y_hat = compute_cost(X, Y, parameters, nn_key, is_train)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    # optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # For saving / restoring sesison when training for long\n",
    "    epoch_counter = tf.get_variable('epoch_counter', shape=[], initializer=tf.zeros_initializer)\n",
    "    counter_op = tf.assign_add(epoch_counter, 1)\n",
    "    saver = tf.train.Saver(max_to_keep=max_to_keep)\n",
    "    \n",
    "    # Calculate the correct predictions\n",
    "    predict_op = tf.argmax(Y_hat, 1)\n",
    "    correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "\n",
    "    # Calculate accuracy on the test set\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    # For impementation of batch norm the tf.GraphKeys.UPDATE_OPS dependency needs to be added\n",
    "    # see documentation on tf.contrib.layers.batch_norm\n",
    "#     update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess: #, tf.control_dependencies(update_ops):\n",
    "        \n",
    "        if (restore_session == False and path.exists(save_session_path)):\n",
    "            raise FileExistsError('Session already exists, either restore the session, or manually delete the files.')\n",
    "        \n",
    "        # restore the previous session if the path already exists\n",
    "        if (model_path != None and restore_session==True):\n",
    "            print(\"Restoring session...\\n\")\n",
    "            saver.restore(sess, model_path)\n",
    "            print(\"Previous epoch counter: %i\\n\\n\" % epoch_counter.eval())\n",
    "        else:\n",
    "            sess.run(init)\n",
    "        \n",
    "        tf.train.export_meta_graph(model_path + '.meta') # save the model file (.meta) only once\n",
    "        \n",
    "        print(\"Cost at start: %f\" % cost.eval({X: X_train, Y: Y_train, is_train: False}))\n",
    "        print(\"Dev cost: %f\" % cost.eval({X: X_dev, Y: Y_dev, is_train: False}))\n",
    "        \n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train, is_train: False})\n",
    "        dev_accuracy = accuracy.eval({X: X_dev, Y: Y_dev, is_train: False})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Dev Accuracy:\", dev_accuracy)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                \n",
    "                try:\n",
    "\n",
    "                    # Select a minibatch\n",
    "                    (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                    # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                    # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                    _,minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y, is_train: True})\n",
    "\n",
    "                    epoch_cost += minibatch_cost / num_minibatches\n",
    "                \n",
    "                # Implement early stopping mechanism on KeyboardInterrupt\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"KeyboardInterrupt received. Stopping early\")\n",
    "                    plot_costs(np.squeeze(costs), np.squeeze(dev_costs), learning_rate, epoch_counter.eval())\n",
    "                    return parameters\n",
    "                \n",
    "            \n",
    "            if (epoch % save_session_interval == 0 and save_session_path != None):\n",
    "                saver.save(sess, model_path, write_meta_graph=False)\n",
    "            \n",
    "            # Save the costs after each epoch for plotting learning curve\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                dev_cost = cost.eval({X: X_dev, Y: Y_dev, is_train: False})\n",
    "                dev_costs.append(dev_cost)\n",
    "                \n",
    "                \n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and (epoch + 1) % 5 == 0:\n",
    "                print (\"\\nCost after epoch %i: %f\" % (epoch + 1, epoch_cost))\n",
    "                print (\"Dev cost after epoch %i: %f\" % (epoch + 1, dev_cost))\n",
    "                \n",
    "                train_accuracy = accuracy.eval({X: X_train, Y: Y_train, is_train: False})\n",
    "                dev_accuracy = accuracy.eval({X: X_dev, Y: Y_dev, is_train: False})\n",
    "                print(\"Train Accuracy:\", train_accuracy)\n",
    "                print(\"Dev Accuracy:\", dev_accuracy)\n",
    "            \n",
    "            # increment the epoch_counter in case the session is saved\n",
    "            # and restored later\n",
    "            sess.run(counter_op)\n",
    "                \n",
    "                \n",
    "        if (save_session_path != None):\n",
    "            saver.save(sess, model_path, write_meta_graph=False)\n",
    "        \n",
    "        \n",
    "        plot_costs(np.squeeze(costs), np.squeeze(dev_costs), learning_rate, epoch_counter.eval())\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train, is_train: False})\n",
    "        dev_accuracy = accuracy.eval({X: X_dev, Y: Y_dev, is_train: False})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Dev Accuracy:\", dev_accuracy)\n",
    "                \n",
    "        return parameters\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at start: 1.097090\n",
      "Dev cost: 1.097712\n",
      "Train Accuracy: 0.27711502\n",
      "Dev Accuracy: 0.28266665\n",
      "\n",
      "Cost after epoch 5: 0.856150\n",
      "Dev cost after epoch 5: 1.045593\n",
      "Train Accuracy: 0.40194932\n",
      "Dev Accuracy: 0.4\n",
      "\n",
      "Cost after epoch 10: 0.685988\n",
      "Dev cost after epoch 10: 1.078506\n",
      "Train Accuracy: 0.43590644\n",
      "Dev Accuracy: 0.44533333\n",
      "\n",
      "Cost after epoch 15: 0.569552\n",
      "Dev cost after epoch 15: 0.975862\n",
      "Train Accuracy: 0.5325926\n",
      "Dev Accuracy: 0.5786667\n",
      "\n",
      "Cost after epoch 20: 0.522651\n",
      "Dev cost after epoch 20: 0.690352\n",
      "Train Accuracy: 0.6983236\n",
      "Dev Accuracy: 0.704\n",
      "\n",
      "Cost after epoch 25: 0.494121\n",
      "Dev cost after epoch 25: 0.455851\n",
      "Train Accuracy: 0.79157895\n",
      "Dev Accuracy: 0.808\n",
      "\n",
      "Cost after epoch 30: 0.467195\n",
      "Dev cost after epoch 30: 0.384743\n",
      "Train Accuracy: 0.84050685\n",
      "Dev Accuracy: 0.824\n",
      "\n",
      "Cost after epoch 35: 0.454066\n",
      "Dev cost after epoch 35: 0.357368\n",
      "Train Accuracy: 0.8659649\n",
      "Dev Accuracy: 0.82133335\n",
      "\n",
      "Cost after epoch 40: 0.429714\n",
      "Dev cost after epoch 40: 0.334755\n",
      "Train Accuracy: 0.88038987\n",
      "Dev Accuracy: 0.8373333\n",
      "\n",
      "Cost after epoch 45: 0.409131\n",
      "Dev cost after epoch 45: 0.321779\n",
      "Train Accuracy: 0.8880312\n",
      "Dev Accuracy: 0.8426667\n",
      "\n",
      "Cost after epoch 50: 0.395239\n",
      "Dev cost after epoch 50: 0.305051\n",
      "Train Accuracy: 0.8938791\n",
      "Dev Accuracy: 0.85866666\n",
      "\n",
      "Cost after epoch 55: 0.377439\n",
      "Dev cost after epoch 55: 0.296205\n",
      "Train Accuracy: 0.8985185\n",
      "Dev Accuracy: 0.85066664\n",
      "\n",
      "Cost after epoch 60: 0.366874\n",
      "Dev cost after epoch 60: 0.280285\n",
      "Train Accuracy: 0.902885\n",
      "Dev Accuracy: 0.85866666\n",
      "\n",
      "Cost after epoch 65: 0.358942\n",
      "Dev cost after epoch 65: 0.269495\n",
      "Train Accuracy: 0.90553606\n",
      "Dev Accuracy: 0.864\n",
      "\n",
      "Cost after epoch 70: 0.346960\n",
      "Dev cost after epoch 70: 0.266696\n",
      "Train Accuracy: 0.91040933\n",
      "Dev Accuracy: 0.8613333\n",
      "\n",
      "Cost after epoch 75: 0.335696\n",
      "Dev cost after epoch 75: 0.257500\n",
      "Train Accuracy: 0.91477585\n",
      "Dev Accuracy: 0.872\n",
      "\n",
      "Cost after epoch 80: 0.322504\n",
      "Dev cost after epoch 80: 0.242333\n",
      "Train Accuracy: 0.91812867\n",
      "Dev Accuracy: 0.88\n",
      "\n",
      "Cost after epoch 85: 0.318420\n",
      "Dev cost after epoch 85: 0.231618\n",
      "Train Accuracy: 0.92265105\n",
      "Dev Accuracy: 0.888\n",
      "\n",
      "Cost after epoch 90: 0.306092\n",
      "Dev cost after epoch 90: 0.229095\n",
      "Train Accuracy: 0.92553604\n",
      "Dev Accuracy: 0.89066666\n",
      "\n",
      "Cost after epoch 95: 0.295977\n",
      "Dev cost after epoch 95: 0.221942\n",
      "Train Accuracy: 0.928538\n",
      "Dev Accuracy: 0.896\n",
      "\n",
      "Cost after epoch 100: 0.292777\n",
      "Dev cost after epoch 100: 0.213170\n",
      "Train Accuracy: 0.93263155\n",
      "Dev Accuracy: 0.904\n",
      "\n",
      "Cost after epoch 105: 0.285466\n",
      "Dev cost after epoch 105: 0.202168\n",
      "Train Accuracy: 0.93668616\n",
      "Dev Accuracy: 0.9173333\n",
      "\n",
      "Cost after epoch 110: 0.272471\n",
      "Dev cost after epoch 110: 0.190255\n",
      "Train Accuracy: 0.939961\n",
      "Dev Accuracy: 0.9253333\n",
      "\n",
      "Cost after epoch 115: 0.265886\n",
      "Dev cost after epoch 115: 0.185045\n",
      "Train Accuracy: 0.94237816\n",
      "Dev Accuracy: 0.928\n",
      "\n",
      "Cost after epoch 120: 0.263088\n",
      "Dev cost after epoch 120: 0.173708\n",
      "Train Accuracy: 0.9476803\n",
      "Dev Accuracy: 0.93866664\n",
      "\n",
      "Cost after epoch 125: 0.253022\n",
      "Dev cost after epoch 125: 0.164648\n",
      "Train Accuracy: 0.94881094\n",
      "Dev Accuracy: 0.94666666\n",
      "\n",
      "Cost after epoch 130: 0.249043\n",
      "Dev cost after epoch 130: 0.159061\n",
      "Train Accuracy: 0.9505263\n",
      "Dev Accuracy: 0.9493333\n",
      "\n",
      "Cost after epoch 135: 0.238587\n",
      "Dev cost after epoch 135: 0.156217\n",
      "Train Accuracy: 0.9519688\n",
      "Dev Accuracy: 0.9493333\n",
      "\n",
      "Cost after epoch 140: 0.240044\n",
      "Dev cost after epoch 140: 0.152729\n",
      "Train Accuracy: 0.9526316\n",
      "Dev Accuracy: 0.9573333\n",
      "\n",
      "Cost after epoch 145: 0.231362\n",
      "Dev cost after epoch 145: 0.145105\n",
      "Train Accuracy: 0.9537232\n",
      "Dev Accuracy: 0.96\n",
      "\n",
      "Cost after epoch 150: 0.230819\n",
      "Dev cost after epoch 150: 0.139108\n",
      "Train Accuracy: 0.9553216\n",
      "Dev Accuracy: 0.96533334\n",
      "\n",
      "Cost after epoch 155: 0.225526\n",
      "Dev cost after epoch 155: 0.147142\n",
      "Train Accuracy: 0.9562573\n",
      "Dev Accuracy: 0.9626667\n",
      "\n",
      "Cost after epoch 160: 0.221765\n",
      "Dev cost after epoch 160: 0.138203\n",
      "Train Accuracy: 0.9579337\n",
      "Dev Accuracy: 0.9626667\n",
      "\n",
      "Cost after epoch 165: 0.219237\n",
      "Dev cost after epoch 165: 0.137598\n",
      "Train Accuracy: 0.95820665\n",
      "Dev Accuracy: 0.9626667\n",
      "\n",
      "Cost after epoch 170: 0.214799\n",
      "Dev cost after epoch 170: 0.132116\n",
      "Train Accuracy: 0.9594542\n",
      "Dev Accuracy: 0.96533334\n",
      "\n",
      "Cost after epoch 175: 0.211262\n",
      "Dev cost after epoch 175: 0.135697\n",
      "Train Accuracy: 0.9613645\n",
      "Dev Accuracy: 0.96\n",
      "\n",
      "Cost after epoch 180: 0.209232\n",
      "Dev cost after epoch 180: 0.134208\n",
      "Train Accuracy: 0.96198833\n",
      "Dev Accuracy: 0.96533334\n",
      "\n",
      "Cost after epoch 185: 0.202282\n",
      "Dev cost after epoch 185: 0.130556\n",
      "Train Accuracy: 0.96249515\n",
      "Dev Accuracy: 0.96533334\n",
      "\n",
      "Cost after epoch 190: 0.198579\n",
      "Dev cost after epoch 190: 0.137572\n",
      "Train Accuracy: 0.962807\n",
      "Dev Accuracy: 0.96\n",
      "\n",
      "Cost after epoch 195: 0.198976\n",
      "Dev cost after epoch 195: 0.132439\n",
      "Train Accuracy: 0.96339184\n",
      "Dev Accuracy: 0.96\n",
      "\n",
      "Cost after epoch 200: 0.195378\n",
      "Dev cost after epoch 200: 0.139917\n",
      "Train Accuracy: 0.9633138\n",
      "Dev Accuracy: 0.9626667\n",
      "\n",
      "Cost after epoch 205: 0.192904\n",
      "Dev cost after epoch 205: 0.125919\n",
      "Train Accuracy: 0.9642885\n",
      "Dev Accuracy: 0.9626667\n",
      "\n",
      "Cost after epoch 210: 0.189256\n",
      "Dev cost after epoch 210: 0.125282\n",
      "Train Accuracy: 0.96499026\n",
      "Dev Accuracy: 0.9626667\n",
      "\n",
      "Cost after epoch 215: 0.187684\n",
      "Dev cost after epoch 215: 0.130921\n",
      "Train Accuracy: 0.96643275\n",
      "Dev Accuracy: 0.96\n",
      "\n",
      "Cost after epoch 220: 0.185136\n",
      "Dev cost after epoch 220: 0.125875\n",
      "Train Accuracy: 0.9663548\n",
      "Dev Accuracy: 0.9626667\n",
      "\n",
      "Cost after epoch 225: 0.181174\n",
      "Dev cost after epoch 225: 0.123263\n",
      "Train Accuracy: 0.9669006\n",
      "Dev Accuracy: 0.9626667\n",
      "\n",
      "Cost after epoch 230: 0.179664\n",
      "Dev cost after epoch 230: 0.126440\n",
      "Train Accuracy: 0.96803117\n",
      "Dev Accuracy: 0.9546667\n",
      "\n",
      "Cost after epoch 235: 0.183661\n",
      "Dev cost after epoch 235: 0.119620\n",
      "Train Accuracy: 0.9663158\n",
      "Dev Accuracy: 0.9626667\n",
      "\n",
      "Cost after epoch 240: 0.178141\n",
      "Dev cost after epoch 240: 0.124655\n",
      "Train Accuracy: 0.9681482\n",
      "Dev Accuracy: 0.9573333\n",
      "\n",
      "Cost after epoch 245: 0.177055\n",
      "Dev cost after epoch 245: 0.126600\n",
      "Train Accuracy: 0.96846\n",
      "Dev Accuracy: 0.9546667\n",
      "\n",
      "Cost after epoch 250: 0.174351\n",
      "Dev cost after epoch 250: 0.122969\n",
      "Train Accuracy: 0.9697466\n",
      "Dev Accuracy: 0.952\n",
      "\n",
      "Cost after epoch 255: 0.171204\n",
      "Dev cost after epoch 255: 0.119290\n",
      "Train Accuracy: 0.9693567\n",
      "Dev Accuracy: 0.952\n",
      "\n",
      "Cost after epoch 260: 0.168790\n",
      "Dev cost after epoch 260: 0.124451\n",
      "Train Accuracy: 0.97048736\n",
      "Dev Accuracy: 0.9546667\n",
      "\n",
      "Cost after epoch 265: 0.166066\n",
      "Dev cost after epoch 265: 0.117669\n",
      "Train Accuracy: 0.9706433\n",
      "Dev Accuracy: 0.952\n",
      "\n",
      "Cost after epoch 270: 0.167304\n",
      "Dev cost after epoch 270: 0.121416\n",
      "Train Accuracy: 0.9709552\n",
      "Dev Accuracy: 0.9493333\n",
      "\n",
      "Cost after epoch 275: 0.163701\n",
      "Dev cost after epoch 275: 0.115948\n",
      "Train Accuracy: 0.97091615\n",
      "Dev Accuracy: 0.9573333\n",
      "\n",
      "Cost after epoch 280: 0.167241\n",
      "Dev cost after epoch 280: 0.124915\n",
      "Train Accuracy: 0.9722027\n",
      "Dev Accuracy: 0.94666666\n",
      "\n",
      "Cost after epoch 285: 0.159644\n",
      "Dev cost after epoch 285: 0.117277\n",
      "Train Accuracy: 0.9722807\n",
      "Dev Accuracy: 0.952\n",
      "\n",
      "Cost after epoch 290: 0.156743\n",
      "Dev cost after epoch 290: 0.117440\n",
      "Train Accuracy: 0.9726316\n",
      "Dev Accuracy: 0.9493333\n",
      "\n",
      "Cost after epoch 295: 0.156938\n",
      "Dev cost after epoch 295: 0.121889\n",
      "Train Accuracy: 0.9725536\n",
      "Dev Accuracy: 0.952\n",
      "\n",
      "Cost after epoch 300: 0.153265\n",
      "Dev cost after epoch 300: 0.117004\n",
      "Train Accuracy: 0.97247565\n",
      "Dev Accuracy: 0.9546667\n",
      "\n",
      "Cost after epoch 305: 0.154288\n",
      "Dev cost after epoch 305: 0.118274\n",
      "Train Accuracy: 0.97387916\n",
      "Dev Accuracy: 0.9493333\n",
      "\n",
      "Cost after epoch 310: 0.153551\n",
      "Dev cost after epoch 310: 0.119876\n",
      "Train Accuracy: 0.97407407\n",
      "Dev Accuracy: 0.94666666\n",
      "\n",
      "Cost after epoch 315: 0.151305\n",
      "Dev cost after epoch 315: 0.119283\n",
      "Train Accuracy: 0.97465885\n",
      "Dev Accuracy: 0.9493333\n",
      "\n",
      "Cost after epoch 320: 0.150028\n",
      "Dev cost after epoch 320: 0.120394\n",
      "Train Accuracy: 0.9748928\n",
      "Dev Accuracy: 0.9493333\n",
      "\n",
      "Cost after epoch 325: 0.150291\n",
      "Dev cost after epoch 325: 0.123020\n",
      "Train Accuracy: 0.97333336\n",
      "Dev Accuracy: 0.9546667\n",
      "\n",
      "Cost after epoch 330: 0.146617\n",
      "Dev cost after epoch 330: 0.123361\n",
      "Train Accuracy: 0.9754776\n",
      "Dev Accuracy: 0.9493333\n",
      "\n",
      "Cost after epoch 335: 0.144940\n",
      "Dev cost after epoch 335: 0.127780\n",
      "Train Accuracy: 0.9752047\n",
      "Dev Accuracy: 0.9546667\n",
      "\n",
      "Cost after epoch 340: 0.146886\n",
      "Dev cost after epoch 340: 0.112703\n",
      "Train Accuracy: 0.97532165\n",
      "Dev Accuracy: 0.952\n",
      "\n",
      "Cost after epoch 345: 0.140318\n",
      "Dev cost after epoch 345: 0.116118\n",
      "Train Accuracy: 0.97586745\n",
      "Dev Accuracy: 0.9493333\n",
      "\n",
      "Cost after epoch 350: 0.145093\n",
      "Dev cost after epoch 350: 0.114775\n",
      "Train Accuracy: 0.9765692\n",
      "Dev Accuracy: 0.94666666\n",
      "\n",
      "Cost after epoch 355: 0.141483\n",
      "Dev cost after epoch 355: 0.111278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.97676414\n",
      "Dev Accuracy: 0.9493333\n",
      "\n",
      "Cost after epoch 360: 0.141160\n",
      "Dev cost after epoch 360: 0.117455\n",
      "Train Accuracy: 0.977232\n",
      "Dev Accuracy: 0.94666666\n",
      "\n",
      "Cost after epoch 365: 0.135125\n",
      "Dev cost after epoch 365: 0.112906\n",
      "Train Accuracy: 0.9773879\n",
      "Dev Accuracy: 0.9493333\n",
      "\n",
      "Cost after epoch 370: 0.137728\n",
      "Dev cost after epoch 370: 0.117344\n",
      "Train Accuracy: 0.9782456\n",
      "Dev Accuracy: 0.952\n",
      "\n",
      "Cost after epoch 375: 0.138612\n",
      "Dev cost after epoch 375: 0.125593\n",
      "Train Accuracy: 0.9779337\n",
      "Dev Accuracy: 0.952\n",
      "\n",
      "Cost after epoch 380: 0.133496\n",
      "Dev cost after epoch 380: 0.117837\n",
      "Train Accuracy: 0.97820663\n",
      "Dev Accuracy: 0.9546667\n",
      "\n",
      "Cost after epoch 385: 0.132045\n",
      "Dev cost after epoch 385: 0.108487\n",
      "Train Accuracy: 0.9777778\n",
      "Dev Accuracy: 0.9573333\n",
      "\n",
      "Cost after epoch 390: 0.131996\n",
      "Dev cost after epoch 390: 0.119406\n",
      "Train Accuracy: 0.97875243\n",
      "Dev Accuracy: 0.9493333\n",
      "\n",
      "Cost after epoch 395: 0.128732\n",
      "Dev cost after epoch 395: 0.119744\n",
      "Train Accuracy: 0.9793762\n",
      "Dev Accuracy: 0.9493333\n",
      "\n",
      "Cost after epoch 400: 0.131906\n",
      "Dev cost after epoch 400: 0.118061\n",
      "Train Accuracy: 0.9791813\n",
      "Dev Accuracy: 0.9546667\n",
      "\n",
      "Cost after epoch 405: 0.129627\n",
      "Dev cost after epoch 405: 0.113723\n",
      "Train Accuracy: 0.9791033\n",
      "Dev Accuracy: 0.9573333\n",
      "\n",
      "Cost after epoch 410: 0.128332\n",
      "Dev cost after epoch 410: 0.111175\n",
      "Train Accuracy: 0.97980505\n",
      "Dev Accuracy: 0.9546667\n",
      "\n",
      "Cost after epoch 415: 0.125764\n",
      "Dev cost after epoch 415: 0.110781\n",
      "Train Accuracy: 0.9797661\n",
      "Dev Accuracy: 0.9546667\n",
      "\n",
      "Cost after epoch 420: 0.124413\n",
      "Dev cost after epoch 420: 0.122097\n",
      "Train Accuracy: 0.9808577\n",
      "Dev Accuracy: 0.9573333\n",
      "\n",
      "Cost after epoch 425: 0.128010\n",
      "Dev cost after epoch 425: 0.120874\n",
      "Train Accuracy: 0.9808187\n",
      "Dev Accuracy: 0.9573333\n",
      "\n",
      "Cost after epoch 430: 0.123613\n",
      "Dev cost after epoch 430: 0.114242\n",
      "Train Accuracy: 0.98136455\n",
      "Dev Accuracy: 0.96\n",
      "\n",
      "Cost after epoch 435: 0.123691\n",
      "Dev cost after epoch 435: 0.112353\n",
      "Train Accuracy: 0.98124754\n",
      "Dev Accuracy: 0.9573333\n",
      "\n",
      "Cost after epoch 440: 0.122616\n",
      "Dev cost after epoch 440: 0.110235\n",
      "Train Accuracy: 0.9807797\n",
      "Dev Accuracy: 0.9573333\n",
      "\n",
      "Cost after epoch 445: 0.120375\n",
      "Dev cost after epoch 445: 0.111917\n",
      "Train Accuracy: 0.9811306\n",
      "Dev Accuracy: 0.9573333\n",
      "\n",
      "Cost after epoch 450: 0.118697\n",
      "Dev cost after epoch 450: 0.108691\n",
      "Train Accuracy: 0.9819493\n",
      "Dev Accuracy: 0.96\n",
      "\n",
      "Cost after epoch 455: 0.116781\n",
      "Dev cost after epoch 455: 0.116561\n",
      "Train Accuracy: 0.98276806\n",
      "Dev Accuracy: 0.9573333\n",
      "\n",
      "Cost after epoch 460: 0.120389\n",
      "Dev cost after epoch 460: 0.112523\n",
      "Train Accuracy: 0.98214424\n",
      "Dev Accuracy: 0.9546667\n",
      "\n",
      "Cost after epoch 465: 0.116659\n",
      "Dev cost after epoch 465: 0.122949\n",
      "Train Accuracy: 0.9823782\n",
      "Dev Accuracy: 0.9573333\n",
      "\n",
      "Cost after epoch 470: 0.115481\n",
      "Dev cost after epoch 470: 0.109617\n",
      "Train Accuracy: 0.98296297\n",
      "Dev Accuracy: 0.9573333\n",
      "\n",
      "Cost after epoch 475: 0.115352\n",
      "Dev cost after epoch 475: 0.120455\n",
      "Train Accuracy: 0.9834308\n",
      "Dev Accuracy: 0.96\n",
      "\n",
      "Cost after epoch 480: 0.114607\n",
      "Dev cost after epoch 480: 0.111674\n",
      "Train Accuracy: 0.9836257\n",
      "Dev Accuracy: 0.9573333\n",
      "\n",
      "Cost after epoch 485: 0.112464\n",
      "Dev cost after epoch 485: 0.127775\n",
      "Train Accuracy: 0.9837817\n",
      "Dev Accuracy: 0.9546667\n",
      "\n",
      "Cost after epoch 490: 0.112655\n",
      "Dev cost after epoch 490: 0.117549\n",
      "Train Accuracy: 0.9836257\n",
      "Dev Accuracy: 0.9573333\n",
      "\n",
      "Cost after epoch 495: 0.114019\n",
      "Dev cost after epoch 495: 0.117533\n",
      "Train Accuracy: 0.98331386\n",
      "Dev Accuracy: 0.9546667\n",
      "\n",
      "Cost after epoch 500: 0.110920\n",
      "Dev cost after epoch 500: 0.115926\n",
      "Train Accuracy: 0.9836647\n",
      "Dev Accuracy: 0.9546667\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9836647\n",
      "Dev Accuracy: 0.9546667\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_dev, Y_dev,\n",
    "                   learning_rate=0.00002,\n",
    "                   num_epochs=500,\n",
    "                   minibatch_size=128,\n",
    "                   save_session_path='train/dataset-512-1/cnn11_lr-0.00002_mbs-128/',\n",
    "                   model_file='model',\n",
    "                   restore_session=False,\n",
    "                   save_session_interval=10,\n",
    "                   nn_key='cnn11')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and restoring saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, session_path, model_file, Y_test_onehot=None):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    checkpoint_path = session_path\n",
    "    model_path = session_path + model_file\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        loader = tf.train.import_meta_graph(model_path)\n",
    "        loader.restore(sess, tf.train.latest_checkpoint(checkpoint_path))\n",
    "\n",
    "        graph = tf.get_default_graph()\n",
    "\n",
    "        X = graph.get_tensor_by_name('X:0')\n",
    "        Y = graph.get_tensor_by_name('Y:0')\n",
    "        is_train = graph.get_tensor_by_name('is_train:0')\n",
    "        \n",
    "        epoch_counter = graph.get_tensor_by_name('epoch_counter:0')\n",
    "        print(epoch_counter.eval())\n",
    "\n",
    "        Y_hat = graph.get_tensor_by_name('softmax_output:0')\n",
    "\n",
    "        predict_op = tf.argmax(Y_hat, 1)\n",
    "\n",
    "        y_hat_test = predict_op.eval({X: X_test, is_train: False})\n",
    "        \n",
    "        # print the accuracy of the test set if the labels are provided\n",
    "        if (Y_test_onehot is not None):\n",
    "            y_test = np.argmax(Y_test_onehot, 1)\n",
    "            print('Accuracy: %f' % (y_hat_test == y_test).mean())\n",
    "        \n",
    "\n",
    "    return y_hat_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_voting(X_test_voting, session_path, model_file):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    checkpoint_path = session_path\n",
    "    model_path = session_path + model_file\n",
    "    \n",
    "    y_hat_test_voting = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        loader = tf.train.import_meta_graph(model_path)\n",
    "        loader.restore(sess, tf.train.latest_checkpoint(checkpoint_path))\n",
    "\n",
    "        graph = tf.get_default_graph()\n",
    "\n",
    "        X = graph.get_tensor_by_name('X:0')\n",
    "        is_train = graph.get_tensor_by_name('is_train:0')\n",
    "\n",
    "        Y_hat = graph.get_tensor_by_name('softmax_output:0')\n",
    "\n",
    "        predict_op = tf.argmax(Y_hat, 1)\n",
    "        \n",
    "        classname, idx, counts = tf.unique_with_counts(predict_op)\n",
    "        predict_voting_op = tf.gather(classname, tf.argmax(counts))\n",
    "\n",
    "        # no. of training examples with the original feature size\n",
    "        m = X_test_voting.shape[0]\n",
    "        \n",
    "        # no. of split training examples of each original example\n",
    "        m_each = X_test_voting.shape[1]\n",
    "        \n",
    "        for ex in range(m):\n",
    "            x_test_voting = make_dimensions_compatible(X_test_voting[ex])\n",
    "            pred = predict_voting_op.eval({X: x_test_voting, is_train: False})\n",
    "            \n",
    "            y_hat_test_voting.append(pred)\n",
    "\n",
    "    return y_hat_test_voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from train/dataset-1024-6/cnn6_lr-0.00002_mbs-128/model\n",
      "500.0\n",
      "Accuracy: 0.994286\n",
      "\n",
      "Predicted values:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "Actual values:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(X_dev, 'train/dataset-1024-6/cnn6_lr-0.00002_mbs-128/', 'model.meta', Y_test_onehot=Y_dev)\n",
    "\n",
    "print(\"\\nPredicted values:\")\n",
    "print(predictions)\n",
    "\n",
    "print(\"\\nActual values:\")\n",
    "print(np.argmax(Y_test, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating test set for accuracy with voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from train/dataset-1024-5/cnn6_lr-0.00002_mbs-128/model\n",
      "Accuracy with voting: 0.960000\n"
     ]
    }
   ],
   "source": [
    "testfile = dataset_relative_path + 'testset_voting_1024.h5'\n",
    "session_path = 'train/dataset-1024-5/cnn6_lr-0.00002_mbs-128/'\n",
    "model_file = 'model.meta'\n",
    "\n",
    "with h5.File(testfile, 'r') as testfile:\n",
    "    X_test_voting = testfile['X']\n",
    "    X_test_voting = np.array(X_test_voting) / 1000\n",
    "    y_test_voting = np.array(testfile['Y'])\n",
    "    \n",
    "    y_hat_test_voting = predict_voting(X_test_voting, session_path, model_file)\n",
    "    \n",
    "    print(\"Accuracy with voting: %f\" % (y_test_voting == y_hat_test_voting).mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(y_hat_test_voting)\n",
    "print(y_test_voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([98.775, 98.975, 98.925,  0.   , 98.725, 98.65 , 98.525, 98.025])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([[95.67, 90.7, 93.3, 93.3],\n",
    "         [95.7, 89.8, 99.2, 93.6],\n",
    "         [96, 89.3, 93.3, 93.6],\n",
    "         [0, 0, 0, 0],\n",
    "         [96.27, 90.9, 93.3, 93.9],\n",
    "         [94.9, 91.4, 95.2, 96.5],\n",
    "         [96.27, 89, 93.3, 93.8],\n",
    "         [93.9, 97.8, 97.3, 98.1]\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([100,100,100,100,98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.6"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([100,100,100,100,98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
